\documentclass[letterpaper,11pt]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.75cm,bottom=2.5cm,nohead]{geometry}
%%% \pdfoutput=1
%%% Disable hyperref arXiv submission:
%%% \usepackage{color,hyperref,graphicx}

\usepackage{color,graphicx}

\usepackage[breakable]{tcolorbox}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%% Basic Macros and TeX2page Conversion:

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Not needed for the LaTeX version:
\def\mathax#1{$#1$}

%%% No footnotes in journal submission:
\def\footnote#1{{}}

%%% Turn off the proofreading macros:
\def\noop#1{{}}
\def\yesir#1{{}}
\def\maybe#1{{}}
\def\nosir#1{{}}

%%% Color highlights if supported: 
\def\colorred#1{{\color{red}#1}}
\def\colorblu#1{{\color{blue}#1}}
\def\colorgru#1{{\color{green}#1}}

%%% Disable hyperref in journal submission:
\def\urlh#1{{}}

%%% Reverse hacks for HTMLified TeX2page:
\def\endash{--}
\def\emdash{---}
\def\etal{{\em{et al}}}
\def\tilde{{\textasciitilde{}}}

%%% Fix the errant footnote numbering:
\makeatletter
\def\footnotextnonum{\xdef\@thefnmark{}\@footnotetext}
\makeatother

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Biological Blueprints for Next Generation AI Systems}

%%% Leave this field blank for dates:

\date{}

\author{Thomas Dean$^{1,2}$\\
Chaofei Fan$^{2}$\\
Francis E. Lewis$^{2}$\\
Megumi Sano$^{2}$}\footnotextnonum{Affiliations: $^{1}$Brown University, $^{2}$Stanford University}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/FRONTMATTER.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{gobble}

\begin{titlepage}

  \maketitle

  \begin{abstract}
% 
    Diverse subfields of neuroscience have enriched artificial intelligence for many decades. With recent advances in machine learning and artificial neural networks, many neuroscientists are partnering with AI researchers and machine learning experts to analyze data and construct models. This paper attempts to demonstrate the value of such collaborations by providing examples of how insights derived from neuroscience research are helping to develop new machine learning algorithms and artificial neural network architectures. We survey the relevant neuroscience necessary to appreciate these insights and then describe how we can translate our current understanding of the relevant neurobiology into algorithmic techniques and architectural designs. Finally, we characterize some of the major challenges facing current AI technology and suggest avenues for overcoming these challenges that draw upon research in developmental and comparative cognitive neuroscience.
%
  \end{abstract}

\end{titlepage}

\newpage

\pagenumbering{roman}

\tableofcontents

\newpage

\pagenumbering{arabic}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% .File: /inputs/INTRODUCTION.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Artificial neural networks support distributed computations in which concepts are represented as patterns of activity in the units that comprise the network layers, and inference is carried out by propagating activation levels between layers weighted by learned connection weights.  Artificial neural networks provide a type of fast, flexible computing well suited to handling ambiguity of the sort we routinely encounter in real-world environments, and, by doing so, they complement traditional symbolic computing technologies.

Engineers frequently borrow ideas from nature and generally find it more practical to translate these ideas into current technology rather than attempt to reproduce nature's solutions in detail. Indeed, the basic idea of artificial neural networks has been implemented multiple times using different technologies in order to approximate the connectivity patterns and signal transmission characteristics of real neural circuits while largely ignoring the physiology of real neurons in their implementation. 

The human brain supports a wide array of learning and memory systems. Some we have begun to understand functionally and behaviorally, others we can only hypothesize must exist, and still others about which we haven't a clue. Just knowing {\it{that}} the brain supports a particular capability can serve as an important clue in engineering complex AI systems. Knowing {\it{how}} can lead to an innovative design, enhanced performance and extended competence. In particular, knowing something about how specific biological circuits relate to behavior helps in designing novel network architectures.

We are interested in designing neural network architectures that leverage what is known about biological information processing to solve complex real-world problems. To focus our efforts, we have set out to design end-to-end systems that assist human programmers in writing, debugging and modifying software. We benefit considerably from working closely with scientists from diverse subdisciplines of neuroscience to seek solutions to specific problems and identify additional problems we may have overlooked. The following section explains why this commingling of people, ideas and technologies is so valuable to us in pursuit of our objectives.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/NEUROSCIENCE.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neuroscience}
\label{section_neuroscience}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

From the brain of an Etruscan shrew weighing in at less than a tenth of a gram to a sperm whale brain weighing more than eight kilograms, it is clear that natural selection has stumbled on a basic brain plan and set of developmental strategies that enables it to construct a diverse set of special-purpose brain architectures for efficiently expressing a wide range of sophisticated behavior~\cite{DouglasandMartinCURRENT-BIOLOGY-12,WillemetBRAIN-SCIENCE-12}. The human brain with its approximately 100 billion neurons and the shrew brain with approximately 1 million neurons share the same basic architecture.

The mouse brain has homologues of most human subcortical nuclei and has contributed significantly to our understanding of the human brain and human neurodegenerative disease in particular. The differences between between human and chimpanzee brains are subtle~\cite{Mora-BermudezetalELIFE-16} and yet humans display a much wider range of behavior and express a much larger repertoire of genes than any other species~\cite{HawrylyczetalNATURE-NEUROSCIENCE-15}. So what makes the difference?

It's the connections between neurons that matter or, more generally, it's the different types of communication between neurons that biologists refer to as {\it{pathways}}. There are electrical, chemical and genetic pathways and each of them obey different constraints and are used for different purposes. They include point-to-point and broadcast methods of communication~\cite{HanetalNATURE-18}. They transfer information at different speeds and using different coding strategies. Layered architectures are common not just in the cortex but throughout the brain. It's the wiring that sets humans apart.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DISTRIBUTED FUNCTION
\subsection{Connectivity}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Human_Brain_Neocortex_Function}{\ref{fig_necortex}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[height=200pt]{./figures/Human_Brain_Neocortex_Function.jpg} 
  \end{center}
%
  \caption{A highly stylized rendering of the major functional areas of the human cortex shown from the side with the head facing to left. Highlighted regions include the occipital lobe shown in shades of green including the primary visual cortex; the parietal lobe shown in shades of blue, including the primary somatosensory cortex; the temporal lobe shown in shades of yellow including the primary auditory cortex; and the frontal lobe shown in shades of pink, including the primary motor and prefrontal cortex. The region outlined by a dashed line on the left is Broca's area and it is historically associated with the production of speech and hence its position relative to the motor cortex. The region outlined by a dashed line on the right is Wernicke’s area and it is historically associated with the understanding of speech and hence its position relative to the sensory cortex. Broca's and Wernicke's areas are found only in the dominant hemisphere which is usually the left as shown here.}
%%%  \caption{A highly stylized rendering of the major functional areas of the human cortex shown from the side with the head facing to right. Highlighted regions include the occipital lobe shown in shades of green including the primary visual cortex; the parietal lobe shown in shades of blue, including the primary somatosensory cortex; the temporal lobe shown in shades of yellow including the primary auditory cortex; and the frontal lobe shown in shades of pink, including the primary motor and prefrontal cortex. The region outlined by a dashed line on the right is Broca’s area and it is historically associated with the production of speech and hence its position relative to the motor cortex. The region outlined by a dashed line on the left is Wernicke’s area and it is historically associated with the understanding of speech and hence its position relative to the sensory cortex.}
%
  \label{fig_necortex}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~{\urlh{#fig_Human_Brain_Neocortex_Function}{\ref{fig_necortex}}} shows the major functional areas of the human neocortex including the primary and secondary sensory and motor areas. The proximal locations of the areas provide a very rough idea of how different functions might relate to another. The graphic shown belies the fact that the brain is three dimensional and much of its functional circuitry hidden under the cortical sheet. The human cerebral cortex is complexly folded to fit within the skull with much of it hidden within the folds. This folded sheet of tissue accounts for more than 75\% of the human brain by volume~\cite{SwansonTiN-95} and is largely responsible for the rich behavioral repertoire that humans exhibit. It is worth pointing out in this context that the cortical sheet enshrouds a collection of evolutionarily preserved and highly specialized circuits homologues of which are found in all mammals and without which the cortex would be useless.

The graphic shown in Figure~{\urlh{#fig_Human_Brain_Neocortex_Function}{\ref{fig_necortex}}} is a simplification of the standard medical textbook diagram. In particular, several of the association areas are not shown and those that are shown are labeled somewhat differently than is common practice. The organizing biological principle is that, the further away from the primary sensory areas, associative functions become more general by integrating information from multiple modalities to construct abstract representations tailored to serve ecologically relevant objectives~\cite{Higher_Cortical_Functions_Association}. It is worth contemplating the arrangement of areas to note that they converge on locations in the cortex that will play an important role in decision making and higher-order cognition more generally.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Human_Brain_Atlas_Allen_Institute}{\ref{fig_brains}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[height=150pt]{./figures/Human_Brain_Atlas_Allen_Institute.jpg} %%% 1806 × 799 pixels
  \end{center}
%
  \caption{Two 3-D renderings of the human brain generated by the Allen Institute {\urlh{ttp://human.brain-map.org/static/brainexplorer}{Brain Explorer}} from the Allen Human Brain Reference Atlas~\cite{HawrylyczetalNATURE-12}. The inset shown in the left upper corner of each panel indicates the orientation of the head. The left panel ({\colorred{A}}) shows 3-D reconstructions of several subcortical nuclei featured in this paper. A cross-sectional view of the whole brain is projected on the mid-sagittal plane dividing the right and left sides of the brain illustrating how the cortex envelopes the subcortical regions. The right panel ({\colorred{B}}) shows the same subcortical nuclei as seen from above (horizontal plane) and to the rear of the brain illustrating how the thalamus is located between the cortical sheet and the subcortical nuclei serving in its role as a relay between the two regions.}
%
  \label{fig_brains}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~{\urlh{#fig_Human_Brain_Atlas_Allen_Institute}{\ref{fig_brains}}} highlights the 3-D structure of several subcortical nuclei emphasized in this paper showing how they relate anatomically to one another and to the cortex. The reconstructions were generated from data generated by {\it{functional magnetic resonance imaging}} (fMRI) of adult human subjects~\cite{HawrylyczetalNATURE-12} and offer additional functional insight complementing conventional histological studies~\cite{BridgeandClarePTRS-B-06}. They don't however provide detailed information concerning either local or long-range connectivity.

Traditionally, tracing connections between individual neurons has been accomplished using a variety of techniques including conventional histological and staining techniques, electrophysiology, neurotropic retroviruses and transgenic organisms expressing fluorescent proteins. However, these methods yield relatively sparse reconstructions and don't scale well to large tissue samples~\cite{Arenkiel2014neural,CallawayCURRENT-OPINION-08}.

Small samples of neural tissue can be fixed, stained and sliced into thin sections. Each of the sections is then scanned with an electron microscope and the resulting digital images analyzed with computer vision software to reconstruct neurons and identify synapses~\cite{MikulaandDenkNATURE-METHODS-15}. The process is time consuming but can be fully automated and scaled to handle larger samples~\cite{JanuszewskietalNATURE-METHODS-18,ZhengetalCELL-18}. 

It is also possible to reconstruct the major {\it{white matter tracts}} formed by bundles of myelinated fibers using diffusion-weighted fMRI and averaging over multiple subjects after registering the individual brain scans with a reference atlas~\cite{OishietalNEUROIMAGE-08,WakanaetalRADIOLOGY-04}. Unlike the previous technologies, this method is not destructive so it can be applied to human subjects and accuracy is improved by averaging over multiple subjects after registering the individual brain scans with a reference atlas

These major tracts increase the speed of signal transmission between regions allowing for more distant communication in larger brains. The differences between the neocortex in humans and chimpanzees are subtle~\cite{Mora-BermudezetalELIFE-16}; however, white matter connections observed in humans but not in chimpanzees particularly link multimodal areas of the temporal, lateral parietal, and inferior frontal cortices, including tracts important for language processing~\cite{ArdeschetalPNAS-19,Gomez-RoblesetalPNAS-15}.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_White_Matter_Tracts_Long_Distance}{\ref{fig_tracts}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[height=150pt]{./figures/White_Matter_Tracts_Long_Distance.jpg} %%% 1873 × 810 pixels
  \end{center}
%
  \caption{White matter tracts corresponding to bundles of myelinated neurons speed the transmission of information between distant regions of the brain. The left panel ({\colorred{A}}) shows the connections between the prefrontal cortex and circuits in the parietal and temporal cortex that shape conscious awareness, guide attention and support short-term memory maintenance~\cite{ChicaetalBSF-18,Dehaene2014}. The parietal and temporal cortices are known for being home to {\it{association areas}} that integrate information from multiple sensory systems thereby creating rich representations necessary for abstract thinking. In humans, white matter tracts between the frontal cortex and the cerebellum {\emdash{}} shown in the right panel ({\colorred{B}}) {\emdash{}} facilitate higher-order cognitive function in addition to their role in supporting motor function in all mammals. For example, these connections are particularly important in the development of reading skills in children and adolescents~\cite{TravisetalHBM-15,KozioletalCEREBELLUM-14}.}
%    
  \label{fig_tracts}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The cerebellum in mammals serves to shape motor activities selected in the basal ganglia by ensuring they are precisely timed and well-coordinated. Such activities are initiated by the basal ganglia with executive oversight from the prefrontal cortex. In humans, the cerebellum also supports cognitive functions such as those involved in reading~\cite{TravisetalHBM-15}. Figure~{\urlh{#fig_White_Matter_Tracts_Long_Distance}{\ref{fig_tracts}}} ({\colorred{B}}) shows the white matter tracts connecting the cerebellum and prefrontal cortex where such abstract cognitive functions originate. 

A white matter bundle called the {\it{arcuate fasciculus}} {\emdash{}} Figure~{\urlh{#fig_White_Matter_Tracts_Long_Distance}{\ref{fig_tracts}}} ({\colorred{A}}) {\emdash{}} provides reciprocal connections between the frontal cortex and association areas in the parietal and temporal lobes plays a key role in attention and the active maintenance of short-term working memory, including support for language processing in the left hemisphere and visuospatial processing in the right hemisphere~\cite{ChicaetalBSF-18}.

The human brain exhibits structure at many scales, the white matter tracts being but one example. A common pattern involves paths that connect multiple circuits that have their own internal components and connections. At a global scale, processing begins in primary sensory areas, propagates forward through dorsal regions integrating additional sources of information to produce composite representations that are processed in the frontal cortex before returning through ventral regions responsible for motivation and action selection.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% RECURRENT FEEDBACK
\subsection{Reciprocity}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many of the connections within such paths are reciprocal allowing feedback to adjust behavior and improve prediction. Similar reflective and self-corrective patterns arise within subcortical regions including the hippocampal complex and basal ganglia, e.g., between the dentate gyrus and CA1 in the hippocampus and as layered networks inside individual subcomponents such as the mossy fiber network within the dentate gyrus. Each level solves different problems, offers general insights and provides hints about how one might realize such solutions in artificial systems.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%% Figure~{\urlh{#fig_Broadmann_Basal_Ganglia_Hippocampus}{\ref{fig_broadman}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[height=150pt]{./figures/Brodmann_Basal_Ganglia_Hippocampus.jpg} %%% 2610 × 1290 pixels
  \end{center}
%
  \caption{The left panel ({\colorred{A}}) illustrates the reciprocal connections between two subnuclei of the basal ganglia, the {\it{putamen}} and {\it{caudate nucleus}}, and locations in prefrontal cortex responsible for influencing action selection. The distinctions between frontal, parietal and temporal cortical areas provide only a very general indication of how their function relates to that of the basal ganglia. The right panel ({\colorred{B}}) highlights reciprocal connections between cortical regions {\emdash{}} identified by the Brodmann areas 7, 8, 9, 11, 12, 13, 19, 20, 21, 22, 23 and 46 {\emdash{}} and the hippocampal complex via the adjacent perirhinal (blue) and the parahippocampal (red) areas. The indicated Brodmann areas generally provide a more nuanced understanding of their possible function than does simply stipulating the cortical lobe they reside in.}
%    
  \label{fig_broadman}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~{\urlh{#fig_Brodmann_Basal_Ganglia_Hippocampus}{\ref{fig_broadman}}} describes how subcortical nuclei such as the hippocampal complex and basal ganglia interact with cortical regions. Such attributions provide insight on how to construct complex artificial neural architectures composed of simpler subnetworks ostensibly responsible for component functions including perception, action selection and episodic memory.

Here we consider two levels of granularity: the first is coarse grained relying on major anatomical divisions illustrated in Figure~{\urlh{#fig_Human_Brain_Neocortex_Function}{\ref{fig_necortex}}}. The second is somewhat finer grained and relies on areal divisions based on cytoarchitectural distinctions involving cell types, neural processes including dendrites and axons, and other histological characteristics.

The former generally employs Korbinian Brodmann's decomposition of the cortex into 52 {\urlh{https://en.wikipedia.org/wiki/Brodmann_area}{areas}} published in 1909~\cite{Brodmann1909} and revised several times since then to take advantage of more modern staining and imaging technologies as well as improved methods for functional localization. In many cases, identifying the Brodmann area associated with the endpoint of a neural connection can tell us a good deal about the functional relationship between two brain regions.

The left side of Figure~{\urlh{#fig_Brodmann_Basal_Ganglia_Hippocampus}{\ref{fig_broadman}}} ({\colorred{A}}) highlights the reciprocal connections between two subnuclei of the basal ganglia, the {\it{putamen}} and {\it{caudate nucleus}}, and locations in prefrontal cortex responsible for influencing action selection by adjusting input to the basal ganglia and, by way of the thalamus, locations in the parietal and temporal cortex that provide information about the current state relevant to decision making. 

We can often improve functional descriptions if we localize to specific Brodmann areas. For example, the {\it{orbitofrontal cortex}} (OFC) is located in the prefrontal cortex is a region of the frontal lobes involved in the cognitive process of decision-making. In humans it consists of {\it{Brodmann area 10, 11 and 47}}. It is defined as the part of the prefrontal cortex that receives projections from the medial dorsal nucleus of the thalamus, and is thought to represent emotion and reward in decision making~\cite{BotvinickandAnANIPS-09}. The prefrontal cortex, consisting of Brodmann areas 8, 9, 10, 11, 12, 13, 44, 45, 46 and 47, includes the OFC but covers a wider range of functionality.

The right side of Figure~{\urlh{#fig_Brodmann_Basal_Ganglia_Hippocampus}{\ref{fig_broadman}}} ({\colorred{B}}) highlights reciprocal connections between cortical areas {\emdash{}} Brodmann areas 7, 8, 9, 11, 12, 13, 19, 20, 21, 22, 23 and 46 {\emdash{}} and the hippocampal complex via the adjacent {\it{perirhinal}} cortex (shown as blue connections) and the {\it{parahippocampal}} cortex (shown as red connections) that are involved in representing and recognizing objects and environmental scenes.

The anatomy of the brain and patterns of connectivity linking its major functional areas provide a structural account that derives from and informs function. However, functional analyses relating to human cognition require technologies that are able to record neural activity or its correlates aligned with relevant behavioral features. Non-human model systems often employed when invasive technology is required.

On the one hand, optogenetics, two-photon microscopy and conventional electrophysiology are able to record from and modify the electrical activity of tens to thousands of neurons at the resolution of a few microns. While this represents progress, the coverage is inadequate for many studies, and the methods are, for the most part, limited to non-human subjects due to the invasive nature of their practical application~\cite{DombeckandTankCSH-11,BoydenBIOLOGY-11,ZhangetalNATURE-10,YizharetalNEURON-11}.

Conversely, fMRI can used to study awake, behaving humans performing a wide range of cognitive tasks, but relies on signals that are at best indirectly related to neural activity as in the case of blood oxygen levels, and that are currently limited to spatial resolutions on the order of tens of millimeters and temporal resolutions on the order of hundreds of milliseconds~\cite{GoenseetalFiCN-16,GloverPMC-11,BuxtonetalNEUROIMAGING-04}.

Moreover, the electrical activity of individual neurons is but one marker for neural function. Other pathways including diffuse signaling by way of chemical neuromodulation and genetic activity and protein transport at the cellular level are becoming increasingly important as markers for behavior at multiple time scales~\cite{WangandWangFiP-19}. Despite these limitations, neuroscientists have made considerable progress by combining information from different model systems using multiple recording technologies.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SENSORIMOTOR HIERARCHY
\subsection{Sensorimotor Hierarchy}
\label{subsection_sensorimotor_hierarchy}
%%% \input{./inputs/parts/SENSORIMOTOR_HIERARCHY.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/SENSORIMOTOR_HIERARCHY.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%% BEGIN SENSORIMOTOR HIERARCHY %%%%%%%%%%%%%%%%%%%%%

Much of the cortex is in the business of learning representations of concepts relevant to survival\footnote{%
%
  The word {\it{perspicacity}} refers to a clarity of perception that enables one to recognize subtle differences between similar physical objects or abstract concepts. It is employed in this context to call attention to the fact that attention, exploration, perception, and prediction are inextricably linked in complex biological systems~\cite{RaoandBallardNATURE-NEUROSCIENCE-99,BarlowNC-89,BaddeleyQJoEP-86,ClarkBBS-13}.}.
%
Perception is the means by which we apprehend and act on the physical realization of the concepts we have learned. It seems obvious that perception serves action. It may not seem so obvious that action serves perception, but the fact is we are almost always moving our head, hands and torso in order to resolve ambiguities in what we see, feeling the shape of unfamiliar objects in order to grasp them firmly and twisting about to see who is behind us calling our name or to get a better idea of where we've come from in order to ensure we can retrace our steps. These are complex sensorimotor activities we depend on every day.

In thinking about physically realizable concepts we think first about what they look, feel, sound and smell like. The sensory cortex is responsible for constructing a hierarchy of representations to characterize such concepts, not to capture everything we sense, but rather to account for what we need to know about concepts to survive. Reconstructing scenes with photographic realism is not what our sensory systems were designed for. Circuits of the primary sensory cortex feed into the circuits of the (unimodal) association sensory cortex that feed into (multimodal) sensory cortex. All of these representations are abstract and yet patterns of regionalization are remarkably preserved within species~\cite{ChenetalNEURON-18,PortuguesetalNEURON-14,KolsteretalJoN-09}.

Concepts arise in patterns of neural activity that account for what we need to know about them, including how they appear to us so we can recognize them, what affordances they offer for us to make use of them and how we might predict their occurrence in decision making. Many of the concepts that are represented in our brains serve to model the dynamics of physical systems that we interact with every day, such as riding a bike, working with tools, opening doors, negotiating stairs and riding escalators in department stores. Just as important, if not more so, are the social dynamics we deal with at work and school with their constantly shifting personal relationships and status rankings. 

If you are a software engineer designing robot control systems, you might give action much the same scrutiny as perception and build a parallel hierarchy of representations that describes the concepts that relate to movement including navigation, articulation and manipulation ranging from servo-motor commands to strategies for moving furniture, but designing or learning these hierarchies independently is generally a bad idea. In mammals, these two hierarchies are tightly coupled to account for how they depend on one another~\cite{FusterPREFRONTAL-CORTEX-15}.

Indeed, determining what sensory representations to learn depends upon and influences what motor representations to learn and {\it{vice versa}}, where we follow the convention of using the term {\it{motor}} as a catchall term for concepts relating to muscles and movement. As pointed out in the introduction, there is evidence to suggest that circuits occurring early in the ventral visual stream code for object-selective features and exhibit large-scale organization characterized by the high-level properties of animacy and object size~\cite{KonkleandCaramazzaJoN-13,LongetalPNAS-18}.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Coupled_Sensory_Motor_Hierarchy}{\ref{fig_coupled}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[width=200pt]{./figures/Coupled_Sensory_Motor_Hierarchy.jpg} %%% 1555 × 635 pixels
  \end{center}
%
  \caption{A simplified block diagram of the cortex. The column on the left represents the posterior cortex including the occipital, temporal and parietal lobes. The column on the right represents the frontal lobe of the cortex corresponding to the primary motor cortex, premotor cortex (association motor cortex) and prefrontal cortex. Green arrows represent interaction with the environment, black arrows represent sensorimotor abstractions and red arrows indicate cognitive activity relating speech, planning and abstract thinking. See the main text for more detail. Adapted from Figure~8.9 in~\cite{FusterPREFRONTAL-CORTEX-15-CHAPTER_8}}
%    
  \label{fig_coupled}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~{\urlh{#fig_Coupled_Sensory_Motor_Hierarchy}{\ref{fig_coupled}}} is a simplified block diagram of the cortex organized as two columns. The left column represents the posterior cortex consisting of the occipital, temporal and parietal lobes that are primarily concerned with processing sensory information. The relevant brain areas are summarized in three blocks roughly corresponding to primary sensory cortex, unimodal association cortex and multimodal association cortex stacked so the least abstract concepts are on the bottom and most abstract on the top. The combined area is often referred to as {\it{semantic memory}} and characterized as long-term declarative memory~\cite{BinderandDesaiTiCS-11}. 

The right column represents the frontal lobe of the cortex corresponding to the primary motor cortex, premotor cortex (associative motor cortex) and prefrontal cortex. The primary motor cortex is responsible for creating abstract representations of motor activity throughout the body. The premotor cortex is responsible for integrating sensory and motor abstractions to construct sensorimotor representations. The prefrontal cortex orchestrates cognitive behavior including speech, planning and abstract thinking, and is reciprocally connected to the association areas just mentioned as well subcortical structures including the basal ganglia and hippocampus.

The two columns are connected with one another at multiple levels: by physical interaction with the environment (green arrows), by sensorimotor abstraction and alignment (black arrows), and by cognitive effort in directing activity mediated through subcortical structures (red arrows). This arrangement supports the formation of rich representations that serve a wide range of cognitive function. The sensorimotor connections and feedback through the environment provide an inductive bias to guide learning, ground inference and reduce sample complexity by reducing reliance on labeled data and enabling opportunities for unsupervised learning~\cite{BarlowNC-89}.

Simple as this model of cortical function may seem, it may be one of the most important architectural contributions of neuroscience to the development of artificial intelligence patterned after the human brain. Some of the lessons have already been integrated into the discipline of control theory through exposure to early work in biological cybernetics~\cite{FukushimaBC-80,Lettvinetal59,Jackson1958selected,GibsonPERCEPTION-50,McCullochandPitts43,vonUexk1926theoretical}, but some of the most important lessons impact the application of machine learning in building autonomous embodied systems including robots and digital assistants as alluded to above. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%% END SENSORIMOTOR HIERARCHY %%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ACTION SELECTION
\subsection{Basal Ganglia}
\label{subsection_basal_ganlia}
%%% \input{./inputs/parts/BASAL_GANGLIA_MOTOR.tex}

%%% File: /inputs/parts/BASAL_GANGLIA_MOTOR.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN BASAL GANGLIA %%%%%%%%%%%%%%%%%%%%%%%%%%

There is a long history of neuroscientists constructing computational models of human cognition~\cite{McClelland79,McClellandandRumelhartPR-88,LebiereandAndersonCSS-93,OReillySCIENCE-06,BotvinickPTRS_B-07}. Different modeling tools make different assumptions and support different levels of detail from rule-based systems to spiking neurons~\cite{OReillyetalLEABRA-16,OReillyetalCCN-12,RasmussenetalPLoS-ONE-17,Eliasmith2013,BlouwandEliasmithCSS-13,JilketalJETAI-08}. In this paper, we are primarily concerned with computational models that leverage ideas from neuroscience to develop AI systems for practical problems. In this subsection and the next, we take a closer look at the basal ganglia and hippocampus using models from neuroscience that reveal computational principles we can apply in a wide range of practical problems.

In contrast with the relative simplicity of the neocortical architecture, the {\it{basal ganglia}} consist of specialized subcortical nuclei that are related by their evolved function. In the following, we emphasize and simplify some of those nuclei and ignore others to focus the discussion and simplify the biology. The basal ganglia provide the basis for motor activity controlled by circuits in the brainstem and conserved throughout vertebrate evolution for nearly half a billion years. The cerebral cortex has been around in the form of a six-layer sheet tiled with a repeating columnar structure since the early mammals came on the scene in the Jurassic period about 200 million years ago. Our lineage separated from mice around 100 million and from macaques and other old world monkeys around 25 million years ago. The modern human neocortex owes much to these earlier evolutionary innovations but is different in ways that make possible our facility with language, complex social organization and sophisticated abstract thinking. Compared with the basal ganglia, the neocortex is structurally elegant and functionally general.

The basal ganglia have evolved along with our neocortex to provide us with a powerful thinking machine, while at the same time leaving us to make do with some less-than-ideal adaptations. We can simulate a conventional computer in our heads but are limited to fewer than a dozen memory registers. Most of us can't perform long division in our heads even though we might know the algorithm and aided with paper and pencil carry out the necessary computations to produce an answer. We rely on the same basic cognitive machinery we use to list a few names in alphabetical order to perform all sorts of more complicated cognitive tasks. Even simpler, however, is the basic operation of choosing one of several actions to perform next. The basal ganglia play a key role in supporting action selection and it is worth looking at in a little more detail in order get a handle on some of the parts of the brain that figure prominently in human cognition. Recall that the cortex is a sheet of neural tissue more or less homogeneous in terms of its local structure quite unlike almost any other part of the brain except for the cerebellar cortex. The cortex sits on top of a structure called the {\it{thalamus}} which among other things serves as a relay in passing information back and forth between the cortex and various subcortical nuclei.

The basal ganglia consist of a bunch of circuits, of varied size, sometimes but not always consisting primarily of one cell type, sometimes but not necessarily compactly clustered together, sporting projections that seem to wander off aimlessly, but more or less located above the brainstem and below the cortex. As a general principle, if a signal sets off along some path exiting from a circuit, then expect some derivative of that signal to appear later reentering the circuit to serve as feedback. Everything about the brain, and your entire body for that matter, has to be carefully regulated to maintain a dynamic state of equilibrium, and unlike human designs, evolution is generally not able to cleanly separate the parts of the circuit that perform computations in service to behavior from those that deal with respiration, immune response, waste removal, cell repair, death and regeneration, etc. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Basal_Ganglia_Anatomy_and_Physiology}{\ref{fig_basal}}}
\begin{figure}
%
  \begin{center}
    \includegraphics[width=4.0in]{./figures/Basal_Ganglia_Anatomy_and_Physiology.jpg} %%% 2194 × 803 pixels @ 72 per inch
  \end{center}
%
  \caption{The left panel ({\colorred{A}}) provides a highly stylized anatomical drawing of the basal ganglia. Figure~\ref{fig_broadman} ({\colorred{A}}) provides more anatomical detail while the above drawing abstracts from the structural detail in order to simplify the functional account. The block diagram shown in the right panel ({\colorred{B}}) depicts the primary components involved in action selection as functional blocks. The blocks shown in blue represent components in the {\it{direct path}} and are described in the text proper. The blocks shown in light green with dashed borders represent additional components that contribute to the {\it{indirect path}}. Good explanations of the indirect path are described in O'Reilly~\etal{}~\cite{OReillyetalCCN-12} or Wang~\etal{}~\cite{WangetalNATURE-NEUROSCIENCE-18} and we return to the basal ganglia in the next section when we look at the executive role of the prefrontal cortex in modulating behavior.}
%
  \label{fig_basal}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The basal ganglia are depicted in Figure~{\urlh{#fig_Basal_Ganglia_Anatomy_and_Physiology}{\ref{fig_basal}}} ({\colorred{A}}) taking some artistic license to keep things simple. The thalamus along with another structure called the {\it{striatum}} provide the interface between the cortex and basal ganglia. The striatum is a combination of a number of smaller nuclei that are anatomically and functionally related; they include the Globus Pallidus (GP), Putamen and Caudate Nucleus and aside from their function as part of the striatum, only the GP will figure prominently in our discussion and only one part of it \emdash{} referred to as the {\it{internal}} GP and identified with the "i" subscript to distinguish it from the {\it{external}} part with "e" subscript.

The other players include the Substantia Nigra (SN) which is at one end of the striatum nestled close to the {\it{amygdala}} which is part of the limbic system involved with memory, decision-making and modulating emotional responses, and the Subthalamic Nucleus (STN). You can think of the cortex as integrating sensory and motor information and making suggestions for what action to take next and the amygdala as supplying information pertaining to the possible emotional consequences of taking different actions to be used as input to action selection. Figure~{\urlh{#fig_Basal_Ganglia_Anatomy_and_Physiology}{\ref{fig_basal}}} ({\colorred{B}}) reconfigures these component nuclei into a smaller number of functionally motivated blocks that control two pathways \emdash{} the {\it{direct pathway}} associated primarily with inhibition and consisting of the internal GP and SN and the {\it{indirect pathway}} playing an excitatory role and consisting of the external GP and STN~\cite{OReillyetalCCN-12,WangetalNATURE-NEUROSCIENCE-18},

The lines connecting the functional blocks shown in Figure~{\urlh{#fig_Basal_Ganglia_Anatomy_and_Physiology}{\ref{fig_basal}}} ({\colorred{B}}) imply neural connectivity, with arrows indicating the direction of influence and colors indicating the valence of the influence, green for excitatory and red for inhibitory. In the action selection cycle, the cortex forwards activations that you can think of as suggestions for what action to take next. These suggestions are propagated through the striatum and forwarded along the direct pathway where two stages of inhibitory neurons initially suppress all of the suggestions and propagate signals back the cortex to activate inhibitory neurons that suppress activity at the source. As this cycle continues, an additional process takes place in the indirect path \emdash{} identified with dashed lines \emdash{} that weighs the advantages and disadvantages of the proposed actions taking in information from throughout the cortex and adjusting the inhibitory bias accordingly.

Eventually, one proposal wins out and all of the others are suppressed allowing a single preferred action to be executed. This cycle of exploring the options for acting and then selecting a single action to execute is constantly repeated during your waking hours. Additional machinery in the thalamus and brain stem regulate whether or not to forward suggestions for acting during sleep when your cortex receives no sensory input and hence any suggestions for acting uninformed by sensory input are ill-advised if not outright dangerous. The above description doesn't begin to convey the complexity of what's going on at the level of individual neurons. Suffice it to say that the usual perfunctory summary consisting of "the winner takes all" doesn't begin to do it justice. The subtleties that arise from the way in which the evidence for and against an action proposal is combined, how ties are broken and deciding when enough evaluation is determined sufficient to make a final choice. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END BASAL GANGLIA %%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MEMORY SYSTEMS
\subsection{Hippocampal Complex}
\label{subsection_hippocampus}
%%% \input{./inputs/parts/HIPPOCAMPAL_COMPLEX.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/HIPPOCAMPAL_COMPLEX.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN HIPPOCAMPAL COMPLEX %%%%%%%%%%%%%%%%%%%%%%%

Figure~{\urlh{#fig_Hippocampus_Anatomy_and_Physiology}{\ref{fig_hippo}}} provides a glimpse of how we construct memories of our experience and subsequently retrieve those memories to support a diverse range of cognitive strategies. In this case, the {\it{hippocampus}} will play a central role as did the basal ganglia in the previous example. In the next section, we explore how the basal ganglia work in concert with the hippocampus to support reinforcement learning. For now, our goal is simply to describe the process whereby we consolidate and then encode experience. In doing so we take the opportunity to talk about the process whereby we retrieve memories, reconstruct a version of that past experience to perform counterfactual inference and imagine possibilities that we have never actually experienced. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Hippocampus_Anatomy_and_Physiology}{\ref{fig_hippo}}}
\begin{figure}
%
  \begin{center}
    \includegraphics[width=4.0in]{./figures/Hippocampus_Anatomy_and_Physiology.jpg} %%% 2362 × 900 pixels @ 72 per inch
  \end{center}
%
  \caption{On the left you see a cartoon drawing of the hippocampus and related cortical and subcortical areas. The primary components include the entorhinal cortex or EHC, the dentate gyrus or DG and two hippocampal (out of four) nuclei referred to as CA3 and CA1. Figure~\ref{fig_broadman} provides additional anatomical detail regarding the connections between cortical regions and the perirhinal and parahippocampal areas adjacent to the hippocampus. The block diagram on the right summarizes the component circuits, along with their projections and reciprocal connections.}
%
  \label{fig_hippo}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The name {\it{hippocampus}} like so many biological terms has obscure origins, generally in Latin or Greek and in this case the latter, relating to its shape that looked like a seahorse to some early anatomists. As shown in Figure~{\urlh{#fig_Hippocampus_Anatomy_and_Physiology}{\ref{fig_hippo}}} ({\colorred{A}}) it is primarily comprised of four subnuclei referred to as CA1, CA2, CA3 and CA4, the first two characters in each abbreviation recalling a previous Latin name, {\it{Cornu Ammanonis}} associated with a ram's horn, apparently preferred by even earlier anatomists. These nuclei are capped by the {\it{dentate gyrus}} (DG) at one end and the {\it{fornix}} at the other\footnote{%
%
  The hippocampus plays an important role in the transfer of information from short-term memory to long-term memory during encoding and retrieval stages. These stages need not occur successively, but are broadly divided in the neuronal mechanisms they require or even in the hippocampal areas they activate. According to Michael Gazzaniga, "encoding is the processing of incoming information that creates memory traces to be stored." There are two steps to encoding: acquisition and consolidation. During acquisition, stimuli are committed to short term memory. Consolidation is where the hippocampus along with other cortical structures stabilize an object within long term memory. ({\urlh{https://en.wikipedia.org/wiki/Hippocampal_memory_encoding_and_retrieval}{SOURCE}})}. 

The hippocampus consists of two nearly identical structures, one in each hemisphere, connected where the parallel tracts of the fornix come together at the midline of the brain. The hippocampus is tightly coupled with the {\it{entorhinal cortex}} (EHC) that plays an important role in memory, navigation and our perception of time. Information flows from the EHC to the hippocampus by one of two pathways: either through the DG to CA3 or via reciprocal connections to and from CA1. The EHC also has reciprocal connections to many cortical areas. Figure~{\urlh{#fig_Brodmann_Basal_Ganglia_Hippocampus}{\ref{fig_broadman}}} ({\colorred{A}}) provides additional anatomical detail. 

In the process of creating a new memory, the hippocampus receives input from multiple cortical areas relevant to current experience, consolidates this information in a condensed format that will enable subsequent retrieval and stores the resulting encoding in memory. In retrieving an existing memory, The EHC starts with cortical activity, typically from motor and sensory association areas, and uses this information to reconstruct a previous memory by activating cortical areas corresponding to the original memory. Before describing how we think such creative consolidation and subsequent reconstruction works, a word about why this process is beneficial might be in order.

Almost every stage of memory is fraught with opportunities to alter stored representations of prior experience. Reconstruction is a creative process in which we are more often than not forced to fill in some details that we might think we observed at the time but actually didn't. In the formation of new memories, consolidation can only make do with whatever information about the experience we have gleaned from observation and committed to short-term memory. If you don't rehearse what you've stored in short-term memory then it will quickly fade, losing detail and potentially introducing errors of omission and commission.

The basic algorithm carried out by the hippocampus and entorhinal cortex working together is illustrated in Figure~{\urlh{#fig_Hippocampus_Anatomy_and_Physiology}{\ref{fig_hippo}}} ({\colorred{B}}). There are two basic processes that we consider here: encoding new memories and retrieving old memories. Encoding involves collecting information gleaned from diverse neural activity originating in multiple cortical regions and consolidating~\cite{MorrisetalNEURON-06} this information to construct a compact encoding that serves as a key or index that will enable subsequent stable \emdash{} meaning reliably consistent even in the presence of distracting information~\cite{EisenbergetalSCIENCE-03} \emdash{} retrieval and reconstruction\footnote{% 
%
   Memory consolidation is a category of processes that stabilize a memory trace after its initial acquisition. Consolidation is distinguished into two specific processes, synaptic consolidation, which is synonymous with late-phase long-term potentiation and occurs within the first few hours after learning, and systems consolidation, where hippocampus-dependent memories become independent of the hippocampus over a period of weeks to years. Recently, a third process has become the focus of research, reconsolidation, in which previously-consolidated memories can be made labile again through reactivation of the memory trace. ({\urlh{https://en.wikipedia.org/wiki/Memory_consolidation}{SOURCE}})}.

EHC receives input from all cortical regions in a condensed form and the axons of EHC pyramidal neurons project primarily to the DG but also to CA1. DG then projects to CA3 which plays a particularly important role in encoding and retrieving memories. CA3 is thought to behave as an autoassociative memory shown here as a recursive neural network. The crucial property of an autoassociative memory is that it is able to retrieve an item from memory using only a portion of the information associated with that item\footnote{%
% 
  {\it{Autoassociative memories}} are capable of retrieving a piece of data upon presentation of only partial information. {\it{Hopfield networks}} are recurrent artificial neural networks that have been shown to act as an autoassociative memory since they are capable of remembering data by observing a portion of that data. Hopfield networks can be trained with a variety of different learning methods including Hebbian learning which is often summarized as "neurons that fire together wire together". ({\urlh{https://en.wikipedia.org/wiki/Hopfield_network}{SOURCE}})}. 

The hippocampus serves as an index, storing different patterns of cortical activity and allowing us to retrieve our memories using only a fragment of what we can recall. The recurrent connections of CA3 are thought to enable this sort of creative reconstruction and play a role in both encoding and retrieving memories. CA3 then projects to CA1 and from there back to the EHC completing the loop and thereby providing recurrent activity involving a much larger circuit.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There are a couple of details that are worth pointing out here as they demonstrate both the strengths and weaknesses of human episodic memory. The first concerns the issue of retrieving a complete memory given a partial index and the second concerns how to retrieve a memory when that memory is similar to one or more other memories, at least in the sense that their respective indices are similar to one another. In the model described here, the first issue is handled by the autoassociative network.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Hippocampus_Auto_Associative_Network}{\ref{fig_assoc}}}
\begin{figure}
%
  \begin{center}
    \includegraphics[width=3.25in]{./figures/Hippocampus_Auto_Associative_Network.jpg} %%% 2540 × 887 pixels
  \end{center}
%
  \caption{The three panels shown here represent the autoassociative network representing the function of CA3 in the hippocampus. Connection weights are shown as diagonal lines, e.g., the dotted blue lines shown in the network on the far left represent the connection weights prior to any training. The middle panel represents the network after encoding the stimulus pattern corresponding to the cortical activation of A and C, and the panel on the far right represents the network, when presented with a partial pattern consisting of just C, employing the recurrent connections of the autoassociator to complete the pattern for the original stimulus and using it to reconstruct the corresponding activation of A and C in the cortex.}
%
  \label{fig_assoc}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The triptych shown in Figure~{\urlh{#fig_Hippocampus_Auto_Associative_Network}{\ref{fig_assoc}}} illustrates how the autoassociative network solves this problem. The panel on the far left is meant to indicate the autoassociative network and its initial state. In the middle panel, we assume the input from the dentate gyrus consist of the two sub patterns A and C and illustrates the reciprocal connections that would be strengthened were we to train the network with this composite pattern of activity. CA3 is responsible for encoding these memory specific patterns of activity for all of our memories.  The panel on the far right is intended to illustrate how given a partial pattern, in this case just one of the two representative patterns that comprise the composite pattern shown in the middle panel, is able to reconstruct the other representative pattern by using the trained autoassociative network to first identify and then strengthen the connections in the original composite.

The second detail concerns the possibility that the encodings for two memories are alike enough to be mistaken with one another. A full account of any of the theories explaining how the human brain solves this problem is beyond the scope of what we can go into here but one theory \emdash{} first articulated by David Marr~\cite{MarrandBrindleyPTRS_B-71,WillshawetalPTRS_B-15} \emdash{} posits that, since the dentate gyrus has a larger number of cells than the EHC, its forward projection will tend to produces an expansion recoding in the DG leading to an increase in the separation between the patterns in CA3.


To complete our account of memory retrieval, we look at how the path that started in the EHC loops back to complete a feedback loop that stabilizes the encoding of memories. So far we've seen how an experience represented by a pattern of activity in the cortex is compressed and represented in the entorhinal cortex which projects this pattern onto the cells in the dentate gyrus thereby increasing the separation between competing patterns the results of which are bound together to generate an index. This index is fed to CA3 where it is incorporated in an autoassociative recursive network so that subsequently when a feature of the original stimulus is present in our conscious experience it activates a subset of the original neurons activated in CA3 and the recurrent connections in the autoassociative network reactivate the remaining neurons completing the pattern that was incorporated when the experience was initially encoded in memory.
  
The remaining step involves explaining how the representation in CA3 reactivates the original stimulus. As shown in Figure~{\urlh{#fig_Hippocampus_Anatomy_and_Physiology}{\ref{fig_hippo}}} ({\colorred{B}}), the entorhinal cortex projects to CA1 in addition to the dentate gyrus. When neurons are projected forward to DG and activated in CA3 they are also activated in CA. Since they are activated at the same time, the connections between the neurons in CA3 and CA1 are strengthened by long-term potentiation\footnote{%
%
  In neuroscience, long-term potentiation (LTP) is a persistent strengthening of synapses based on recent patterns of activity. These are patterns of synaptic activity that produce a long-lasting increase in signal transmission between two neurons. The opposite of LTP is long-term depression, which produces a long-lasting decrease in synaptic strength. It is one of several phenomena underlying synaptic plasticity, the ability of chemical synapses to change their strength. As memories are thought to be encoded by modification of synaptic strength, LTP is widely considered one of the major cellular mechanisms that underlies learning and memory. ({\urlh{https://en.wikipedia.org/wiki/Long-term_potentiation}{SOURCE}})}.
%
The result is a stable, sparse, invertible mapping that allows the hippocampus to recreate the original cortical activity patterns during retrieval~\cite{OReillyetalCS-15,McClellandandGoddardHIPPOCAMPUS-97}. Reactivating the same combination of cortical areas as the original stimulus and causing us to reexperience the event as a memory. An additional process called {\it{reconsolidation}} is thought to allow previously-consolidated memories become labile again as a consequence of reactivation\footnote{%
q%
  Memory reconsolidation is the process of previously consolidated memories being recalled and actively consolidated. It is a distinct process that serves to maintain, strengthen and modify memories that are already stored in the long-term memory. Once memories undergo the process of consolidation and become part of long-term memory, they are thought of as stable. However, the retrieval of a memory trace can cause another labile phase that then requires an active process to make the memory stable after retrieval is complete. It is believed that post-retrieval stabilization is different and distinct from consolidation, despite its overlap in function. ({\urlh{https://en.wikipedia.org/wiki/Memory_consolidation}{SOURCE}})}.
%
See {\urlh{box_patterns}{Box~\colorred{A}}} for detail on storing and retrieving memories in the hippocampus.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END HIPPOCAMPAL COMPLEX %%%%%%%%%%%%%%%%%%%%%%%%


%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/boxes/BOX_CHAOFEI_FAN.tex} 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/boxes/BOX_CHAOFEI_FAN.tex

\begin{center}
  \begin{tcolorbox}[breakable,sharp corners=all,coltitle=black,colbacktitle=white,
    width=\textwidth,boxsep=5pt,left=5pt,right=5pt,
    title={\textbf{Box A: Pattern Separation, Completion and Integration}}]

~~~~As discussed in Section~\ref{subsection_hippocampus}, {\it{pattern separation}} reduces the similarity between input patterns of activity by orthogonalizing inputs to minimize interference between patterns and increase hippocampal storage capacity~\cite{KesnerandRollsNBR-15}. Pattern separation involves primarily DG and CA3 {\emdash{}} see Section~\ref{subsection_hippocampus} for an explanation of the acronyms. The DG maps input from EHC to a much larger and sparsely active GC population. In rats, the number of neurons in the DG exceeds that in EHC by about 5:1~\cite{DrewetalLEARNING-MEMORY-13}. This expansion coding with strong inhibitory interneurons and a competitive learning rule can greatly reduce the overlap between inputs. The DG connects to CA3 mainly through {\it{mossy fibers}} that reliably activate CA3 pyramidal neurons and sustain activation for tens of seconds~\cite{VyletaetalELIFE-16}. Each CA3 neuron receives a small number of these connections from DG so the degree of sparsity is maintained~\cite{KesnerandRollsNBR-15}.

~~~~{\it{Pattern completion}} reconstructs the complete stored pattern given a partial input. Each pyramidal neuron in CA3 receives a large number of synapses from other pyramidal cells forming a recurrent network that serves as an autoassociative memory for pattern completion~\cite{KesnerandRollsNBR-15}. During learning, recurrent connections between active CA3 neurons are strengthened and later when neurons encoding part of an episode are reactivated, they recurrently activate other connected cells to reconstruct the original episode. Basket cells in CA3 form inhibitory synapses to pyramidal cells to dampen excitatory responses thereby emphasizing key features~\cite{NeunuebelandKnierimNEURON-14}. 

~~~~Pattern completion provides access to relevant experience to support decision making in novel situations, and while pattern separation helps downstream discrimination, perfectly orthogonal representations are not ideal in the case we want events that occurred close together to have similar representations. In this case, {\it{pattern integration}} represents related experiences as overlapping populations. There are a number of neural mechanisms suggested to support pattern integration in the hippocampus. We consider two here, the first of which involves {\it{neurogenesis}}. 

~~~~There is evidence that hundreds of new GCs are added to an adult human hippocampus everyday~\cite{SpaldingetalCELL-13}, and stronger evidence suggests that thousands of new GCs are added to rodent’s hippocampus, though not all survive~\cite{KitabatakeetalNCNM-07}. Unlike mature GCs that fire sparsely, immature GCs are more active and have lower threshold for induction of long-term potentiation~\cite{AimoneetalNEURON-09,GeetalNATURE-06,Schmidt-HieberetalNATURE-04}. Aimone~\etal{}~\cite{AimoneetalNEURON-09} posit that a population of hyperactive young GCs could collectively encode events close in time to decrease pattern separation in DG. Others hypothesize that neurogenesis may increase storage capacity by protecting old GCs from new information~\cite{BeckerHIPPOCAMPUS-05,WiskottetalHIPPOCAMPUS-06} or that young active GCs could improve the resolution of memory content~\cite{AimoneetalNEURON-11}. 

~~~~Alternatively, pattern integration might be enabled by recurrent connections involving the hippocampus and neocortex. Recurrent connections in the hippocampus, mainly in CA3 region, can replay an entire episode given a part of it. The replayed episode is backprojected to the neocortex through EHC, that can then recirculate the replayed episode as input to hippocampus to trigger replay of another episode that has overlapping elements with previous one. Kumaran~\etal{}~\cite{KumaranetalTiCS-16} propose that this kind replay between hippocampus and neocortical regions can combine representations of elements that seldom occur together but appear in similar contexts. In addition to integrating experiences with shared elements, backprojection to the medial prefrontal cortex (mPFC) may bias hippocampus to reactivate experiences that are more behaviorally relevant~\cite{SchlichtingandPrestonCOiBS} {\emdash{}} see {\urlh{box_memories}{Box~\colorred{B}}} for more on behavioral relevance. The concurrent presentation of these memories in mPFC may further improve the learning of abstraction relations across episodes.

  \end{tcolorbox}
\end{center}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/ARCHITECTURE.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%% File: ./inputs/ARCHITECTURE.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\section{Architecture}
\label{section_architecture}

The architecture of the human brain, at any scale you choose to consider, bears little or no resemblance to conventional computer architectures. There is no separate program memory, no centralized processing unit, no highly stable, random-access, non-volatile memory and nothing like the digital level of abstraction that enables software engineers to ignore instabilities in the analog circuits that implement logic gates. Since representations (data) are collocated with the transformations (computations) that operate on them and different parts of the brain perform different computations requiring different types of memory, the human brain has to support multiple memory systems.

Human memory\footnote{%
%  
  For students looking for an introduction to memory systems in the brain, you might start with Chapter~24 in~\cite{Bearetal2015} if you own or have access to either the 3rd or 4th edition, or, for a more succinct overview, try Rolls~\cite{RollsARP-01}, Raslau~\etal{}~\cite{RaslauetalAJN-15,RaslauetalAJN-15}, or, most expedient, Wikipedia ({\urlh{https://en.wikipedia.org/wiki/Neuroanatomy_of_memory}{URL}}).} 
%
is characterized along several dimensions depending on what sort of information is stored, how it is accessed and how long it remains accessible~\cite{CowanPBR-08}. Short-term, long-term and working memory are differentiated on the basis of access, persistence, volatility and the effort required to maintain. Short term is measured in seconds, long term in days, months or years and working memory is essentially short-term memory that can be maintained (with cognitive effort) indefinitely and manipulated (very roughly) analogous to a register in the ALU of a von Neumann machine~\cite{BaddeleyQJoEP-86}.

Declarative memory is defined by the ability to explicitly (consciously) recollect facts, whereas non-declarative memory is accessed unconsciously or implicitly through performance rather than recollection. Episodic memory is generally considered long-term and declarative, and is further differentiated on the basis of the kinds of relationships it can encode, including spatial, temporal and social~\cite{StachenfeldetalNATURE-17,RueckemannandBuffaloNATURE-2017,KumaranandMaguireJoN-05,NealandEichenbaum1993}. Procedural knowledge, including motor, visuospatial and cognitive skills, is encoded in the cerebellum, the putamen and caudate nucleus of the basal ganglia, the motor cortex, and frontal cortex.

To ground the discussion, we introduce the {\it{programmer's apprentice}} as an example of the sort of digital assistants we envision as an application of the technologies presented in this paper. We consider several core components of the apprentice architecture each of which depends on or implements one or more memory systems. Drawing upon concepts covered earlier, we consider three major elements:
%
\begin{enumerate}
%
\item the role of the posterior cortex role in supporting declarative knowledge and semantic memory,
%
\item the basal ganglia and prefrontal cortex as the basis for motivation and executive function, and
%
\item the hippocampal formation\footnote{%
%
  The {\urlh{https://en.wikipedia.org/wiki/Hippocampal_formation}{hippocampal formation}} is a compound structure located in the medial temporal cortex that consists of the dentate gyrus, the {\urlh{https://en.wikipedia.org/wiki/Hippocampus}{hippocampus}} proper, subiculum and, depending on whom you consult, presubiculum, parasubiculum and {\urlh{https://en.wikipedia.org/wiki/Entorhinal_cortex}{entorhinal cortex}}.} 
%
in supporting episodic memory formation, retrieval and consolidation.
%
\end{enumerate}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% EMBODIED COGNITION
\subsection{Embodied Cognition}
\label{subsection_embodied_cognition}
%%% \input{./inputs/parts/EMBODIED_COGNITION.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/EMBODIED_COGNITION.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Embodied cognition is the theory that an organism's body shapes its understanding of the environment it inhabits and grounds its perception of and interaction with that environment. Importantly, the environment completes a loop that links perception and action enabling the organism to formulate and test predictive models that guide behavior. Such models serve as the foundation for commonsense reasoning and provide a starting point for understanding a much wider range of concrete and abstract systems, giving rise to a tendency in humans to attribute self-styled agency to both animate and inanimate objects.

To ground our discussion, we consider a personal assistant that works with a software engineer in the role of an apprentice learning on the job, as was common in the guilds and trade associations of medieval cities. The programmer's apprentice we imagine here is a novice programmer but has the intuitive skills of an idiot savant, given that the apprentice has a suite of powerful programming tools as an integral part of its brain. These tools constitute the assistant's body, its peripheral nervous system if you will.

The original programmer's apprentice was the name of project initiated at MIT by Chuck Rich and Dick Waters and Howie Shrobe to build an intelligent assistant that would help a programmer to write, debug and evolve software. Our version of the programmer's apprentice is implemented as an instance of a hierarchical neural network architecture. It has a variety of conventional inputs including speech, text and vision, as well as output modalities including the ability to run code and display program output and execution traces. 

The programmer's apprentice relies on multiple sources of input, including dialogue in the form of text utterances, visual information from an editor buffer shared by the programmer and apprentice and information from a fully {\it{instrumented integrated development environment}} (FIDE) designed for analyzing, writing and debugging code adapted to interface seamlessly with the apprentice as we might move our limbs or direct our gaze. As in case of the legs you were born with, the apprentice has to learn how to use its prosthetic extensions.

This input is processed by a collection of neural networks modeled after the primary sensory areas in the primate brain. The outputs of these networks feed into a hierarchy of additional networks corresponding to uni-modal secondary and multi-modal association areas that produce increasingly abstract representations as one ascends the hierarchy as illustrate in Figure~{\urlh{#fig_Posterior_Cortex_Semantic_Cloud_Memory}{\ref{fig_posterior}}}.
  
%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Posterior_Cortex_Semantic_Cloud_Memory}{\ref{fig_posterior}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[width=200pt]{./figures/Posterior_Cortex_Semantic_Cloud_Memory.jpg} % 1106 × 973 pixels
  \end{center}
%
  \caption{The architecture of the apprentice sensory cortex including the layers corresponding to abstract, multi-modal representations handled by the association areas can be realized as a multi-layer hierarchical neural network model consisting of standard neural network components. This graphic depicts these components as encapsulated in thought bubbles of the sort often employed in cartoons to indicate what some cartoon character is thinking. Analogously, the technical term "thought vector" is used to refer to the activation state of the output layer of such a component. All of the bubbles appear to contain networks with exactly the same architecture, where one might expect sensory modality to dictate local architecture. The hierarchical architecture depicted here is modeled after the mammalian neocortex that appears to be tiled with columnar component networks called cortical columns that self-assemble into larger networks and adapt locally to accommodate their input. In practice, it may be necessary to engineer modality-specific networks for the lowest levels of the hierarchy \emdash{} analogous to the primary sensory and motor areas of the neocortex, but more general-purpose networks for the higher levels in the hierarchy \emdash{} analogous to the sensory and motor association areas.}
%
  \label{fig_posterior}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Architecturally, the apprentice's FIDE is an instance of a differentiable neural computer (DNC) introduced by Alex Graves and his colleagues at DeepMind~\cite{GravesetalNATURE-16}. The assistant combined with its FIDE corresponds to a neural network that can read from and write to an external memory matrix, combining the characteristics of a random-access memory and set of memory-mapped device drivers and programmable interrupt controllers. The interface supports a fixed number of commands and channels that provide feedback.

The integrated development environment and its associated software engineering tools constitute an extension of the apprentice’s capabilities in much the same way that a piano or violin extends a musician. The extension becomes an integral part of the person possessing it and over time their brain creates a topographic map that facilitates interacting with the extension. We expect the same to occur in the case of the assistant. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CONSCIOUS ATTENTION
\subsection{Conscious Attention}
\label{subsection_conscious_attention}
%%% \input{./inputs/parts/CONSCIOUS_ATTENTION.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: /inputs/parts/CONSCIOUS_ATTENTION.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Stanislas Dehaene and his colleagues have developed a computational model of consciousness that provides a practical framework for thinking about consciousness that is sufficiently detailed for much of what an engineer might care about in designing digital assistants~\cite{Dehaene2014}. Dehaene's work extends the {\it{Global Workspace}} Theory of Bernard Baars~\cite{Baars1988}. Dehaene's version of the theory combined with Yoshua Bengio's concept of a {\it{consciousness prior}} and deep reinforcement learning~\cite{MnihetalCoRR-13,NairetalCoRR-15} suggest a model for constructing and maintaining the cognitive states that arise and persist during complex problem solving~\cite{BengioCoRR-17}.

Global Workspace Theory accounts for both conscious and unconscious thought with the primary distinction for our purpose being that the former has been selected for attention and the latter has not been so selected. Sensory data arrives at the periphery of the organism. The data is initially processed in the primary sensory areas located in posterior cortex, propagates forward and is further processed in increasingly-abstract multimodal association areas. Even as information flows forward toward the front of the brain, the results of abstract computations performed in the association areas are fed back toward the primary sensory cortex. This basic pattern of activity is common in all mammals.

Humans have a large frontal cortex that includes machinery responsible for conscious awareness and that depends on an extensive network of specialized neurons called {\it{spindle cells}} that span a large portion of the posterior cortex allowing circuits in the frontal cortex to sense relevant activity throughout this area and then manage this activity by creating and maintaining the persistent state vectors that are necessary when generating extended narratives or working on complex problems that require juggling many component concepts at once. Figure~{\urlh{#fig_Global_Workspace_Conscious_Attention}{\ref{fig_conscious}}} suggests a neural architecture combining the idea of a global workspace with that of an attentional system for identifying relevant input.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Global_Workspace_Conscious_Attention}{\ref{fig_conscious}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[height=130pt]{./figures/Global_Workspace_Conscious_Attention.jpg} % 1063 × 487 pixels
  \end{center}
%
  \caption{The basic capabilities required to support conscious awareness can be realized in a relatively simple computational architecture that represents the apprentice's global workspace and incorporates a model of attention that surveys activity throughout somatosensory and motor cortex, identifies the activity relevant to the current focus of attention and then maintains this state of activity so that it can readily be utilized in problem solving.  In the case of the apprentice, new information is ingested into the model at the system interface, including dialog in the form of text, visual information in the form of editor screen images, and a collection of programming-related signals originating from a suite of software development tools. 
%
Single-modality sensory information feeds into multimodal association areas to create rich abstract representations. Attentional networks in the prefrontal cortex take as input activations occurring throughout the posterior cortex. These networks are trained by reinforcement learning to identify areas of activity worth attending to and the learned policy selects a set of these areas to attend to and sustain. This attentional process is guided by a prior that prefers low-dimensional thought vectors corresponding to statements about the world that are either true, highly probable or very useful for making decisions. Humans can sustain only a few such activations at a time. The apprentice need not be so constrained.}
%
  \label{fig_conscious}
%
\end{figure}

These attentional networks are connected to regions throughout the cortex and are trained via reinforcement learning to recognize events worth attending to according to the learned value function. Using extensive networks of connections {\emdash{}} both incoming and outgoing, attentional networks are able to create a composite representation of the current situation that can serve a wide range of executive cognitive functions, including decision making and imagining possible futures. The basic idea of a neural network trained to attend to relevant parts of the input is key to a number of the systems that we'll be looking at.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Global_Workspace_Episodic_Memory}{\ref{fig_episodic}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[height=150pt]{./figures/Global_Workspace_Episodic_Memory.jpg} % 1182 × 636 pixels
  \end{center}
%
  \caption{You can think of the episodic memory encoded in the hippocampus and entorhinal cortex as RAM and the actively maintained memories in the prefrontal cortex as the contents of registers in a conventional von Neumann architecture. Since the activated memories have different temporal characteristics and functional relationships with the contents of the global workspace, we implement them as two separate NTM memory systems each with its own special-purpose controller. Actively maintained information highlighted in the global workspace is used to generate keys for retrieving relevant memories that augment the highlighted activations. While the associative keys required to access locations only partially match locations, they can can still be used to guide attention allowing the NTM to recognize and even partially merge related locations. In general, locations in memory correspond to thought vectors that can be composed with other thought vectors to shape the global context for interpretation.}
%
  \label{fig_episodic}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In their paper~\cite{GravesetalNATURE-16} in {\it{Nature}}, The authors note that "there are interesting parallels between the memory mechanisms of a DNC and the functional capabilities of the mammalian hippocampus. DNC memory modification is fast and can be one-shot, resembling the associative long-term potentiation of hippocampal CA3 and CA1 synapses. The hippocampal dentate gyrus, a region known to support neurogenesis, has been proposed to increase representational sparsity, thereby enhancing memory capacity: usage-based memory allocation and sparse weightings may provide similar facilities." See the discussion of neurogenesis as an algorithmic technique in {\urlh{box_patterns}{Box~\colorred{A}}}.

The global workspace summarizes recent experience in terms of sensory input, its integration, abstraction and inferred relevance to the context in which the underlying information was acquired. To exploit the knowledge encapsulated in such experience, the apprentice must identify and make available relevant experience. The apprentice's experience is encoded as tuples in an NTM that supports associative recall. We'll ignore the details of the encoding process to focus on how episodic memory is organized, searched and applied to solving problems.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ACTIVITY SELECTION
\subsection{Action Selection}
\label{subsection_action_selection}
%%% \input{./inputs/parts/ACTION_SELECTION.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/ACTION_SELECTION.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN COMPLEMENTARY MEMORY %%%%%%%%%%%%%%%%%%%%%%

%%% Solves the problem of initializing / allocating networks that 
%%% span the input space, but that are unlikely to interfere with 
%%% one another and can be gradually integrated with other systems. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In both neuroscience and artifial intelligence, reinforcement learning problems are typically modeled as Markov decision problems (MDPs). While MDPs can be solved in polynomial time, the size of the state space is often prohibitively large, making practical solution intractable~\cite{LittmanetalUAI-95}. Hierarchical reinforcement learning offers a means of reducing the computational burden by decomposing the state space resulting in a relatively small number of tractable MDPs each of which can be solved independently~\cite{KaelblingICML-93,DietterichJAIR-00,HengstEMLDM-17}. However, the problem of finding an optimal decomposition is itself intractable and hence it is necessary to resort heuristic methods and approximate solutions.

There exist a number of approaches that develop solutions to the problem of hierarchical reinforcement learning (HRL) employing various decomposition strategies, several of which we draw inspiration from~\cite{NarasimhanetalJAIR-18,AndreasetalICML-17,SahnietalCoRR-17,KulkarnietalNIPS-16,BakkerandSchmidhuberIAS-04,MoffaertandNoweJMLR-14,PashevichetalCoRR-18} including a few that relate to biological or biologically plausible models~\cite{RasmussenetalPLoS-ONE-17,DiuketalCRMHOB-13,FrankandBadreCEREBRAL-CORTEX-12,RibasFernandesNEURON-11}. It's important to keep in mind that we are dealing a partially-observable, high-dimensional, continuous state space, and an action space that includes abstract cognitive activities in addition to concrete physical activities that engage the motor system in interacting with the environment. 

In the treatment here, we emphasize the problem of life-long learning as it relates to the nonstationarity of underlying process as a consequence of changes in the external environment and changes in the goals of the agent and the neural substrate available for computation during development and extending on into adulthood. In the case of a growing infant, the changes involve the appearance and maturation of critical circuits and the limitations of finite memory. In both human and machine, internal representations progress from concrete to abstract, building on a foundation grounded in the environment. This maturation in cognitive capability is accelerated by a curriculum that takes advantage of dependencies between concepts. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Hippocampus_Inspired_Learning_Redux}{\ref{fig_hippo}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[width=350pt]{./figures/Hippocampus_Inspired_Learning_Redux.jpg} 
  \end{center}
%
  \caption{The network shown here takes as input a pattern of activation originating in the temporal and parietal lobes and selects an action to perform. The subnetworks labeled \colorred{A} and \colorred{B} are relatively straightforward multilayer neural networks that compute features and generate representations as their output. Network \colorred{A} takes as input a representation of the current state, and generates a representation of the context for action selection. Network \colorblu{K} is an embedding network that takes as input a sequence of states corresponding to recent activity and generates as output a unique key associated with a subspace of the full MDP state space that includes the current state. The box labeled \colorblu{M} corresponds to a location in working memory. The networks \colorred{C}, \colorred{D}, \colorred{E} and \colorred{F} are controllers for two differentiable neural computer (DNC) peripherals that provide storage and access for short-term and long-term memory respectively. The long-term memory is used to store the weights for networks that encode architecturally identical networks {\emdash{}} only the weights are different {\emdash{}} providing specialized expertise in restricted domains corresponding to subspaces of the full MDP state space. The model operates in two modes. In each cycle during the {\it{online mode}}, the \colorred{C} controller loads the selected expert network into location \colorblu{M} where it is fed the output of \colorred{A} and produces the input to \colorred{B}. In this mode, the short-term memory is used to record activity traces that are subsequently used in the {\it{offline}} mode to update the networks stored in long-term memory. The network in the lower-right inset implements a version of {\it{pseudo rehearsal}} as a means of mitigating catastrophic forgetting~\cite{FrenchTiCS-99}.}
%
  \label{fig_hippo}
%
\end{figure}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The network model shown in Figure~{\urlh{#fig_Hippocampus_Inspired_Learning_Redux}{\ref{fig_hippo}}} illustrates a system that takes as input a pattern of neural activity originating in the medial temporal and inferior parietal cortex and selects an action to perform. This particular example is meant to illustrate how episodic memory might play an expanded role in action selection. For illustration, patterns of activity serve as proxies for the state of the external environment and are represented in the figure as a sequence $s_t, s_{t-1}, s_{t-2}, ...$. The subnetworks labeled \colorred{A} and \colorred{B} are relatively straightforward multilayer neural networks that compute features and generate representations as their output. Network \colorblu{A} takes as input a representation of the current state, and generates a representation of the {\it{context}} for action selection.

%%% In keeping with our objective to demonstrate how ideas from neuroscience can influence and accelerate the development of artificial intelligence, this model is not intended to emulate how the human brain processes information. Nor is the architecture intended to mirror that of the human brain or any other biological organism. What we borrow from neuroscience are ideas that enable engineers to solve problems that current AI systems cannot handle or do so poorly. We are not bothered by combining neural mechanisms that neuroscientists currently identify with, say, the basal ganglia with neural mechanisms associated with the hippocampal formation. At this stage in the development of more capable AI systems, what engineers need most are general principles that inform design and be applied whenever the need arises.

We'll explain the function of the box labeled \colorblu{M} in a moment; assume for now that it generates a representation of the options available for acting in the current context. Network \colorred{B} then takes these suggestions as input and produces as output a representation of the selected action. The boxes labeled \colorred{C}, \colorred{D}, \colorred{E} and \colorred{F} are controllers for two differentiable neural computer (DNC) units that provide storage and access for short-term and long-term memory respectively. The controllers on the left are part of the online system for selecting actions. The controllers on the right are responsible for off-line training during which the recorded actions, along with their associated states and rewards are consolidated in long-term memory using experience replay.

The blue boxes represent stored information in the form of key-value pairs. Each key is associated with a subset or {\it{subspace}} of the set of all states that represents a restricted domain of expertise for selecting actions. The value for this key is a function implemented as a network trained as an expert for the associated subspace. \colorblu{K} is an embedding network that takes as input a sequence of states corresponding to recent activity and generates as output a unique key associated with a subspace of the current state. A given state can belong to more than one subspace and the particular key selected at any given point in time depends on the current state and the immediately previous states in a fixed window. The order of the states matters. 

In the online phase, the embedding network retrieves this key which it forwards to the controller labeled \colorred{C} that uses it to retrieve the expert for the relevant subspace. The box labeled \colorblu{M} corresponds to a location in working memory and in each online cycle the \colorred{C} controller loads the expert subsystem in location \colorblu{M} of working memory where it can be utilized to compute a set of options appropriate for the current state. During off-line periods the system uses the recorded sequences of activity to run some variant of experience replay to update the relevant expert subsystems stored in long term memory~\cite{AndrychowiczetalCoRR-17,SchauletalCoRR-15,LinML-92}.

The training that occurs offline involves adjusting the weights of networks using relatively small samples and so runs the risk of catastrophic interference in transfer learning~\cite{McClellandetalPR-95}. One way in which we hope to ameliorate the adverse consequences of catastrophic interference is by defining separate networks for separate subspaces. The embedding space method mentioned in describing \colorblu{K} is designed to isolate expertise by identifying states that tend to occur together. The hope is that the actions exercised is such states will tend to be interrelated and hence they should be represented using the same network to facilitate their coordination. 

Of course temporal proximity in their occurrence doesn't guarantee they serve the same task since we are always getting distracted or interrupted requiring us to interleave tasks that have very little to do with one another. It may be possible to segment activity streams into coherent tasks in a similar way to how we segment conversations involving multiple speakers~\cite{SeldinetalICSS-01,SeldinetalICML-01}. Alternatively, there has been some success with the method of {\it{pseudo rehearsal}} which consists of retraining existing networks by interleaving new examples with synthetic-examples produced by randomly activating the existing network~\cite{ZhiyuanandBingLML-18,KirkpatricketalCoRR-16,AnsetalCSS-02,FrenchTiCS-99,FrenchCONNECTION-SCIENCE-97,RobinsCONNECTION-SCIENCE-95}.

In this model the STM roughly corresponds to the hippocampus as the storage system for episodic memory. The LTM resembles the cerebellum in the way that it essentially compiles prior activity to construct a set of programs each of which spans some portion of the overall state space. As described above, the STM is only used for temporary storage awaiting off-line replay to consolidate recent memories. An alternative is to maintain a much larger collection of episodic memories that can be used in a manner similar to that suggested in Gershman and Daw who posit that we routinely draw upon our stored memories in the hippocampus to figure out what to do in novel situations not covered by our other sources of procedural knowledge~\cite{GershmanandDawANNUAL-REVIEWS-17}. See {\urlh{box_memories}{Box~\colorred{B}}} for more detail concerning episodic memory and experience replay.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/boxes/BOX_MEGUMI_SANO.tex} 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/boxes/BOX_MEGUMI_SANO.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
  \begin{tcolorbox}[breakable,sharp corners=all,coltitle=black,colbacktitle=white,
    width=\textwidth,boxsep=5pt,left=5pt,right=5pt,
    title={\textbf{Box B: Replaying Experience, Consolidating Memory}}]
    
~~~~When we encounter a new experience in the environment, we do not act independently of the past, but rather, our past experiences substantially inform our present decisions. Here, we introduce the basic principles of \textit{hippocampal replay} and a few key ways in which it has motivated reinforcement learning algorithms.

~~~~Replay is the process by which hippocampal representations of previous experiences are sequentially reactivated~\cite{CarretalNATURE-NEUROSCIENCE-11}. Studies show that cells in the rodent hippocampus replay past experiences to stabilize behaviorally relevant memories~\cite{OlafsdottirCURRENT-BIOLOGY-18,JooandFrankNATURE-REVIEWS-NEUROSCIENCE-18}. Though initially observed in spatial tasks, recent work suggests that non-spatial task states are also replayed, and that this phenomenon is common in humans~\cite{SchuckandNivDOI-18}. 

~~~~In the reinforcement learning literature, the \textit{experience replay} algorithm was introduced as an analogical framework in online learning agents~\cite{LinML-92}. Transitions containing state, action, and reward information are sequentially stored in memory and sampled randomly for learning. Randomly replaying old memories not only allows decorrelation of consecutive experiences encountered during data collection, but also enables reuse of training data, increasing sample efficiency, and encourages resampling of rare experiences, potentially alleviating forgetting. 

~~~~A relatively well-studied question is \textit{what to replay}. Some studies suggest the correlation of replay frequency with \textit{novelty} approximated by temporal difference (TD) error~\cite{FosterandWilsonNATURE-06}, and others with high \textit{reward}~\cite{OlafsdottirCURRENT-BIOLOGY-18}. In particular, dopaminergic release, which encodes both novelty and reward~\cite{MenegasetalELIFE-17}, enhances {\textit{sharp wave-ripple}} activation \emdash{} the basic unit of replay. Yet other studies show that experiences more \textit{vulnerable to forgetting} are more likely to be replayed~\cite{SchapiroetalNATURE-COMMUNICATIONS-18}. While the exact selection algorithm is unknown, the observed association with novelty inspired the \textit{prioritized experience replay} algorithm which samples experiences with probabilities weighted by their TD errors and is now consistently preferred to the originally proposed uniform sampling variant~\cite{SchauletalCoRR-15}. 

~~~~The significantly less studied question is \textit{what happens during replay}. Besides re-learning of experiences, many neuroscientists support the idea that replay also serves as a substrate for \textit{memory consolidation} \emdash{} the gradual integration of new experiences processed into existing knowledge representations in the neocortex~\cite{WilsonandMcNaughtonSCIENCE-94,McClellandetalPR-95,KarlssonandFrankNATURE-NEUROSCIENCE-09,BendorandWilsonNATURE-NEUROSCIENCE-12,KumaranetalTiCS-16}, as to stabilize memories against interference. The idea is that replaying information stored in memory will encourage synaptic consolidation processes. 

~~~~While we lack a precise understanding of the underlying mechanisms of consolidation in the brain, in our architecture we frame consolidation as the process by which experiences are used to update expert subsystems stored in long-term memory. We propose an adaptive replay algorithm whereby experiences with contexts similar to the current context are replayed and thus preferentially consolidated into long-term storage. Since action selection directly depends on the relevant expert network drawn from long-term memory, we can ensure to maximally update the currently relevant expert network with existing memories related to its corresponding context. This algorithm is partly inspired by the result by~\cite{JooandFrankNATURE-REVIEWS-NEUROSCIENCE-18} whereby they observed that when a rat pauses at a branching point in a maze, it replays representations of trajectories in the past with similar context to drive its present decision-making. 

~~~~There exist many other cognitively inspired variants of experience replay. One example is {\textit{hindsight experience replay}}~\cite{AndrychowiczetalCoRR-17}, where the agent pretends that whatever state it reaches had been the goal state from the start and learns from the experience regardless of whether it actually succeeded, just as humans can learn from undesirable outcomes. 

  \end{tcolorbox}
\end{center}

In this case, the LTM stores what can be thought of as subroutines or libraries for solving routine problems. Used in the manner described in Gershman and Daw~\cite{GershmanandDawANNUAL-REVIEWS-17}, the DNC labeled STM more closely captures the functionality of the hippocampus in combining short-term and long-term episodic memories with specific procedural knowledge based on past experience that may or may not be common enough to warrant compiling as a standalone library. The dentate gyrus is best known for its ability to separate patterns to avoid mistaking one pattern for another. Less well understood is a possible complementary role that involves integrating similar patterns.

The ability to draw upon episodic memory to select what to do in situations similar to those encountered in the past provides a simple form of one-shot learning. It could enable us to make predictions, perform hypothetical reasoning and put ourselves in someone else's shoes assuming that our ability to retrieve memories allows us match situations that we find ourselves that we haven't experienced, but know from someone else's experience. It might avoid some of the problems with interference if the process of integrating new procedural knowledge with old could be spread out over longer periods if, say, each time you encounter a similar situation you make only minor adjustments to the weights of the associated subspace expert network. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END COMPLEMENTARY MEMORY %%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% EXECUTIVE CONTROL
\subsection{Executive Control}
\label{subsection_executive_control}
%%% \input{./inputs/parts/EXECUTIVE_CONTROL.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/EXECUTIVE_CONTROL.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN PREFRONTAL HIERARCHY %%%%%%%%%%%%%%%%%%%%%%

There is a growing consensus and a fair bit of evidence to support the hypothesis that the human frontal cortex is in charge of executive control, goal-directed planning and abstract thinking. There are differences in opinion about how these cognitive processes are implemented and how they coordinate their activities with that of the rest of the brain. One thing that seems clear is that the frontal cortex and in particular the prefrontal cortex employs many of the same strategies as do networks elsewhere in the brain, both cortical and subcortical.

In particular, circuits in the prefrontal cortex recapitulate the coarse-to-fine, concrete-to-abstract feature hierarchies that we see in the sensory, motor and somatosensory cortex. They exhibit the profuse reciprocal recurrent connections between levels of abstraction that enable us to generalize on the basis of relatively small amounts of information, learn to make accurate predictions in an unsupervised manner depending on observations and interactions with the environment to ground our conclusions, and that provide the foundation for constructing a rich repertoire of representations that serve decision-making.

The neural correlates of abstract thinking, including the circuits that enable us to solve practical problems as well as pursue pure mathematics, are generally agreed to be located in the prefrontal cortex with reciprocal connections throughout the rest of the cerebral cortex, the cerebellar cortex and subcortical regions including the basal ganglia, hippocampal formation and parts of the limbic system involved with emotion, motivation and episodic memory. See {\urlh{box_abstract}{Box~\colorred{C}}} for more detail regarding abstraction, hierarchy and executive oversight in the prefrontal cortex.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/boxes/BOX_EUGENE_LEWIS.tex} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
  \begin{tcolorbox}[breakable,sharp corners=all,coltitle=black,colbacktitle=white,
    width=\textwidth,boxsep=5pt,left=5pt,right=5pt,
    title={\textbf{Box B: Replaying Experience, Consolidating Memory}}]
    
~~~~When we encounter a new experience in the environment, we do not act independently of the past, but rather, our past experiences substantially inform our present decisions. Here, we introduce the basic principles of \textit{hippocampal replay} and a few key ways in which it has motivated reinforcement learning algorithms.

~~~~Replay is the process by which hippocampal representations of previous experiences are sequentially reactivated~\cite{CarretalNATURE-NEUROSCIENCE-11}. Studies show that cells in the rodent hippocampus replay past experiences to stabilize behaviorally relevant memories~\cite{OlafsdottirCURRENT-BIOLOGY-18,JooandFrankNATURE-REVIEWS-NEUROSCIENCE-18}. Though initially observed in spatial tasks, recent work suggests that non-spatial task states are also replayed, and that this phenomenon is common in humans~\cite{SchuckandNivDOI-18}. 

~~~~In the reinforcement learning literature, the \textit{experience replay} algorithm was introduced as an analogical framework in online learning agents~\cite{LinML-92}. Transitions containing state, action, and reward information are sequentially stored in memory and sampled randomly for learning. Randomly replaying old memories not only allows decorrelation of consecutive experiences encountered during data collection, but also enables reuse of training data, increasing sample efficiency, and encourages resampling of rare experiences, potentially alleviating forgetting. 

~~~~A relatively well-studied question is \textit{what to replay}. Some studies suggest the correlation of replay frequency with \textit{novelty} approximated by temporal difference (TD) error~\cite{FosterandWilsonNATURE-06}, and others with high \textit{reward}~\cite{OlafsdottirCURRENT-BIOLOGY-18}. In particular, dopaminergic release, which encodes both novelty and reward~\cite{MenegasetalELIFE-17}, enhances {\textit{sharp wave-ripple}} activation \emdash{} the basic unit of replay. Yet other studies show that experiences more \textit{vulnerable to forgetting} are more likely to be replayed~\cite{SchapiroetalNATURE-COMMUNICATIONS-18}. While the exact selection algorithm is unknown, the observed association with novelty inspired the \textit{prioritized experience replay} algorithm which samples experiences with probabilities weighted by their TD errors and is now consistently preferred to the originally proposed uniform sampling variant~\cite{SchauletalCoRR-15}. 

~~~~The significantly less studied question is \textit{what happens during replay}. Besides re-learning of experiences, many neuroscientists support the idea that replay also serves as a substrate for \textit{memory consolidation} \emdash{} the gradual integration of new experiences processed into existing knowledge representations in the neocortex~\cite{WilsonandMcNaughtonSCIENCE-94,McClellandetalPR-95,KarlssonandFrankNATURE-NEUROSCIENCE-09,BendorandWilsonNATURE-NEUROSCIENCE-12,KumaranetalTiCS-16}, as to stabilize memories against interference. The idea is that replaying information stored in memory will encourage synaptic consolidation processes. 

~~~~While we lack a precise understanding of the underlying mechanisms of consolidation in the brain, in our architecture we frame consolidation as the process by which experiences are used to update expert subsystems stored in long-term memory. We propose an adaptive replay algorithm whereby experiences with contexts similar to the current context are replayed and thus preferentially consolidated into long-term storage. Since action selection directly depends on the relevant expert network drawn from long-term memory, we can ensure to maximally update the currently relevant expert network with existing memories related to its corresponding context. This algorithm is partly inspired by the result by~\cite{JooandFrankNATURE-REVIEWS-NEUROSCIENCE-18} whereby they observed that when a rat pauses at a branching point in a maze, it replays representations of trajectories in the past with similar context to drive its present decision-making. 

~~~~There exist many other cognitively inspired variants of experience replay. One example is {\textit{hindsight experience replay}}~\cite{AndrychowiczetalCoRR-17}, where the agent pretends that whatever state it reaches had been the goal state from the start and learns from the experience regardless of whether it actually succeeded, just as humans can learn from undesirable outcomes. 

  \end{tcolorbox}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With respect to hierarchical goal-based planning, there is growing evidence pointing to a set of adjoining regions in the prefrontal cortex that are responsible for how abstract plans are initially selected, subsequently refined and finally realized as concrete actions. These same regions also appear to be involved in relational reasoning from simple binary relations to higher-order relationships.

These theoretical observations combined with behavioral studies and fMRI recordings have led to a number of computational models of hierarchical planning that exhibit similar patterns of cognitive activity. In particular, cognitive neuroscientists have developed models of how such abstract hierarchical reasoning in the prefrontal cortex is related to what we know about how the basal ganglia and areas of the limbic system involved in motivation contribute to action selection~\cite{OReillySCIENCE-06}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Prefrontal_Hierarchy_Biology_Technology}{\ref{fig_prefer}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[height=150pt]{./figures/Prefrontal_Hierarchy_Biology_Graze.jpg} 
  \end{center}
%
  \caption{%
%
    The panel on the left highlights three areas of the prefrontal cortex shown in the figure from left to right (rostro-caudal) and referred to in the text as the {\it{lateral frontal polar cortex}} (\colorred{LFPC}) {\it{dorsolateral prefrontal cortex}} (\colorred{DPFC}) and {\it{anterior premotor cortex}} (\colorred{APMC}). According to the theory first articulated by Joaqu\'{i}n Fuster and subsequently refined David Badre~\cite{BadreandWagnerNEURON-04}, Mark D'Esposito~\cite{DEspositoetalNATURE-95} and Etienne Koechlin~\etal~\cite{KoechlinetalSCIENCE-03} and their colleagues, as actions are specified from abstract plans to concrete responses, progressively posterior regions of lateral frontal cortex are responsible for integrating more concrete information over more proximate time intervals. This process of progressive articulation does not correspond to different stages of execution so much as to how actions are selected, maintained and inhibited at multiple levels of abstraction~\cite{BadreTiCS-08}. The panel on the right shows a simple neural-network model of the brain regions aligned with the rostro-caudal axis of the frontal cortex and hypothesized to account for how action representations are selected, maintained and inhibited at multiple levels of abstraction. The neural-network model is described in more detail in the main text, but a few points are in order here: {\colorred{A}} {\emdash{}} different abstraction layers may include input from other sources, e.g., natural language embeddings, that are only required at particular levels of abstraction; {\colorred{B}} {\emdash{}} each recurrent level of the abstraction hierarchy includes state variables encoding information that would typically appear on the call stack in a conventional computer architecture; {\colorred{C}} {\emdash{}} attentional layers mask (suppress) input that is not determined to be relevant to decision making at a given time and level of abstraction resulting in a sparse context vector.}
%
  \label{fig_prefer}
%
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The network shown on the right in Figure~{\urlh{#fig_Prefrontal_Hierarchy_Biology_Technology}{\ref{fig_prefer}}} consists of three subnetworks that roughly align with the {\urlh{https://en.wikipedia.org/wiki/Frontal_lobe}{lateral frontal polar cortex}} (bottom), {\urlh{https://en.wikipedia.org/wiki/Dorsolateral_prefrontal_cortex}{dorsolateral prefrontal cortex}} (middle) and {\urlh{https://en.wikipedia.org/wiki/Premotor_cortex}{anterior premotor cortex}} (top) as shown in the figure. Each of the subnetworks is composed of three elements: a recurrent multilayer perceptron constructed of interleaved convolutional and max-pooling layers shown in orange, a multilayer attention network shown in green and a masking layer in blue that selectively suppresses a subset of the outputs of the convolutional stack in accordance with the output of the attention network.

Input to each of the three subnetworks includes areas of associative activity throughout the sensory and motor cortex as well as areas corresponding to higher-level abstractions located in the frontal cortex responsible for abstract thought and subcortical regions responsible for motivation. While not emphasized here, the active maintenance in working memory of information originating from these sources\footnote{%
  %
  Susan Courtney provides an excellent overview of the many sources of information that are utilized by cognitive functions supported in the frontal cortex~\cite{CourtneyCABN-04}. In particular, her articulation of the role of attention and cognitive control aligns with the views that we've emphasized in class and that drive our designs:
  %
\begin{quotation}
  %
  [The circuits in the prefrontal cortex that drive goal directed planning and executive control] receive multimodal information about the current environment and have access to previously stored memories. The prefrontal cortex's extensive outputs allow for direct control of motor behavior, but they may also influence behavior indirectly by altering perceptual and cognitive representations and influencing the storage and re- trieval of long-term memories.\\
  I suggest that attention and cognitive control are not directed actions or specific processes contained within any particular set of brain regions. Rather, what we experience and observe that we call attention and cognitive control are emergent properties dependent on the dis- tributed representation of all types of information, both that available from present perceptual input and the information currently sustained in WM, including contextual and motivational information.
%
\end{quotation}}
%
is critical for the cognitive activities that these networks support~\cite{CourtneyCABN-04,Goldman-RakicARN-88}. The outputs are fed to a network (not shown) that serves as the interface for the peripheral motor system (the fully instrumented integrated development environment (FIDE) in the case of the programmer's apprentice) which could play the role of the basal ganglia and cerebellum, but could also be considerably simpler depending on the application.

Figure~{\urlh{#fig_Prefrontal_Hierarchy_Biology_Technology}{\ref{fig_prefer}}} is just a sketch employing familiar neural network components to make the point that building these architectures out of standard components is not the most significant challenge. The real challenge is in training them as part of larger system with lots of moving parts. The expectation here, as in the model sketched in Figure~{\urlh{#fig_Hippocampus_Inspired_Learning_Redux}{\ref{fig_hippo}}}, is that end-to-end training with stochastic gradient descent isn't going to work, and that training will likely require some form of layer-by-layer developmentally-staged curriculum learning~\cite{LampinenetalCoRR-19,GulcehreetalCoRR-16,BengioetalCoRR-15,BengioetalICML-09} and a strategy for holding some weights fixed while adjusting other weights to account for new information and avoid problems like catastrophic forgetting.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END PREFRONTAL HIERARCHY %%%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DIGITAL ASSISTANTS
\subsection{Digital Assistants}
\label{subsection_digital_assistants}
%%% \input{./inputs/parts/DIGITAL_ASSISTANTS.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/DIGITAL_ASSISTANTS.tex

We focus on automated programming for several reasons: deep neural networks have recently demonstrated progress on automated code synthesis and program repair by leveraging existing technologies; computers and, in particular, modern integrated development environments present a rich alternative to the dominant simulation environments; programming is challenging for humans and machines alike and we foresee opportunities to increase the productivity of software engineers. That said, in this section we emphasize basic tools that enable the assistant to encode, represent and manipulate fully differentiable programs.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/parts/PROCEDURAL_MODEL.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN PROCEDURAL MODEL %%%%%%%%%%%%%%%%%%%%%%%%%%

The integrated development environment and its associated software engineering tools constitute an extension of the apprentice’s capabilities in much the same way that a piano or violin extends a musician or a prosthetic limb extends someone who has lost an arm or leg. The extension becomes an integral part of the person possessing it and over time their brain creates a topographic map that facilitates interacting with the extension\footnote{%
%
  In the mammalian brain, information pertaining to sensing and motor control is topographically mapped to reflect the intrinsic structure of that information required for interpretation. This was early recognized in the work of Hubel and Wiesel~\cite{HubelandWieselJoP-68,HubelandWieselJoP-62} on the striate cortex of the cat and macaque monkey and in the work of Wilder Penfield~\cite{PenfieldandBoldreyBRAIN-37} developing the idea of a cortical homunculus in the primary motor and somatosensory areas of the brain located between the parietal and frontal lobes of the primate cortex. Such maps have become associated with the theory of embodied cognition.}. 
 
As engineers designing the apprentice, part of our job is to create tools that enable the apprentice to learn its trade and eventually become an expert. Conventional IDE tools simplify the job of software engineers in designing software. The fully instrumented IDE (FIDE) that we engineer for the apprentice will be integrated into the apprentice’s cognitive architecture so that tasks like stepping a debugger or setting breakpoints are as easy for the apprentice as balancing parentheses and checking for spelling errors in a text editor is for us.

As a first step in simplifying the use of FIDE for coding, the apprentice is designed to manipulate programs as abstract syntax trees (AST) and easily move back and forth between the AST representation and the original source code in collaborating with the programmer. Both the apprentice and the programmer can modify or make references to text appearing in the FIDE window by pointing to items or highlighting regions of the source code. The text and AST versions of the programs represented in the FIDE are automatically synchronized so that the program under development is forced to adhere to certain syntactic invariants. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Differentiable_Structured_Programs}{\ref{fig_programs}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[width=3in]{./figures/Differentiable_Structured_Programs.jpg} % 625 × 528 pixels
  \end{center}
%
  \caption{We use pointers to represent programs as abstract syntax trees and partition the NTM memory, as in a conventional computer, into program memory and a LIFO execution (call) stack to support recursion and reentrant procedure invocations, including call frames for return addresses, local variable values and related parameters. The NTM controller manages the program counter and LIFO call stack to simulate the execution of programs stored in program memory. Program statements are represented as embedding vectors and the system learns to evaluate these representations in order to generate intermediate results that are also embeddings. It is a simple matter to execute the corresponding code in the FIDE and incorporate any of the results as features in embeddings.}
%
  \label{fig_programs}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To support this hypothesis, we are developing distributed representations for programs that enable the apprentice to efficiently search for solutions to programming problems by allowing the apprentice to easily move back and forth between the two paradigms, exploiting both conventional approaches to program synthesis and recent work on machine learning and inference in artificial neural networks. Neural Turing Machines coupled with reinforcement learning are capable of learning simple programs~\cite{GravesetalCoRR-14}. We are interested in representing structured programs expressed in modern programming languages. Our approach is to alter the NTM controller and impose additional structure on the NTM memory designed to support procedural abstraction. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END PROCEDURAL MODEL %%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/parts/EXECUTION_MODEL.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN EXECUTION_MODEL.tex %%%%%%%%%%%%%%%%%%%%%%%

What could we do with such a representation? It is important to understand why we don’t work with some intermediate representation like bytecodes. By working in the target programming language, we can take advantage of both the abstractions afforded by the language and the expert knowledge of the programmer about how to exploit those abstractions. The apprentice is bootstrapped with several statistical language models: one trained on a natural language corpus and the other on a large code repository. Using these resources and the means of representing and manipulating program embeddings, we intend to train the apprentice to predict the next expression in a partially constructed program by using a variant of imagination-based planning~\cite{PascanuetalCoRR-17}. As another example, we will attempt to leverage NLP methods to generate proposals for substituting one program fragment for another as the basis for code completion. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#fig_Differentiable_Program_Emulation}{\ref{fig_emulator}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[width=2.65in]{./figures/Differentiable_Program_Emulation.jpg} % 918 × 781 pixels
  \end{center}
%
  \caption{This slide illustrates how we make use of input / output pairs as program invariants to narrow search for the next statement in the evolving target program. At any given moment the call stack contains the trace of a single conditioned path through the developing program. A single path is unlikely to provide sufficient information to account for the constraints implicit in all of the sample input / output pairs and so we intend to use a limited lookahead planning system to sample multiple execution traces in order to inform the prediction of the next program statement. 
%
These so-called imagination-augmented agents implement a novel architecture for reinforcement learning that balances exploration and exploitation using imperfect models to generate trajectories from some initial state using actions sampled from a rollout policy~\cite{PascanuetalCoRR-17,WeberetalCoRR-17,HamricketalCoRR-17,GuezetalCoRR-18}. These trajectories are then combined and fed to an output policy along with the action proposed by a model-free policy to make better decisions. There are related reinforcement learning architectures that perform Monte Carlo Markov chain search to apply and collect the constraints from multiple input / output pairs.}
%
  \label{fig_emulator}
%
\end{figure}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Differentiable Neural Program (DNP) representation and associated NTM controller for managing the call stack and single-stepping through such programs allow us to exploit the advantages of distributed vector representations to predict the next statement in a program under construction. This model makes it easy to take advantage of supplied natural language descriptions and example input / output pairs plus incorporate semantic information in the form of execution traces generated by utilizing the FIDE to evaluate each statement and encoding information about local variables on the stack.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END EXECUTION_MODEL.tex  %%%%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/parts/PROGRAMMING_MODEL.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN PROGRAMMING MODEL %%%%%%%%%%%%%%%%%%%%%%%%%%%

The imagination-based planning (IBP) for reinforcement learning framework~\cite{PascanuetalCoRR-17} serves as an example for how the code synthesis module might be implemented. The IBP architecture combines three separate adaptive components: (a) the {\it{controller}} + {\it{memory}} system which maps a state $s \in S$ and history $h \in H$ to an action $a \in A$; (b) the {\it{manager}} maps a history $h \in H$ to a route $u \in U$ that determines whether the system performs an action in the {\it{compute}} environment, e.g., single-step the program in the FIDE, or performs an imagination step, e.g., generates a proposal for modifying the existing code under construction; the {\it{imagination model}} is a form of dynamical systems model that maps a pair consisting of a state $s \in S$ and an action $a \in A$ to an imagined next state $s' \in S$ and scalar-valued reward $r \in R$.

The imagination model can be implemented as an interaction network~\cite{BattagliaetalNIPS-16} or using the graph-networks framework~\cite{BattagliaetalCoRR-18,SanchezetalCoRR-18}. The three components are trained by three distinct, concurrent, on-policy training loops. The IBP framework shown in Figure~{\urlh{#Graph_Nets_Imagination_Coding}{\ref{fig_imagine}}} allows code synthesis to alternate between exploiting by modifying and running code, and exploring by using the model to investigate and analyze what would happen if you actually did act. The manager chooses whether to execute a command or predict (imagine) its result and can generate any number of trajectories to produce a tree $h_t$ of imagined results. The controller takes this tree plus the compiled history and chooses an action (command) to carry out in the FIDE.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Figure~{\urlh{#Graph_Nets_Imagination_Coding}{\ref{fig_imagine}}}
\begin{figure}
%
  \begin{center} 
    \includegraphics[width=2.15in]{./figures/Graph_Nets_Imagination_Coding.jpg} % 905 × 1096 pixels
  \end{center}
%
  \caption{The above graphic illustrates how we might adapt the imagination-based planning (IBP) for reinforcement learning framework~\cite{PascanuetalCoRR-17} for use as the core of the apprentice code synthesis module. Actions in this case correspond to transformations of the program under development. States incorporate the history of the evolving partial program. Imagination consists of exploring sequences of program transformations.}
%
  \label{fig_imagine}
%
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END PROGRAMMING MODEL %%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% END-TO-END SYSTEM
\subsection{End-to-End Systems}
\label{subsection_end-to-end_system}
%%% \input{./inputs/parts/END-TO-END_SYSTEM.tex}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% File: ./inputs/parts/END-TO-END_SYSTEM.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN PROGRAMMING MODEL %%%%%%%%%%%%%%%%%%%%%%%%%%%

The subsections that comprise this section of the paper very roughly account for the functional systems of the human brain. Admittedly perception is given short shrift and the systems that deal with emotion and motivation are hardly mentioned at all, the former to save space and the latter because it isn't particularly relevant.

In addition, there was no effort made to map the artificial neural networks described in this section onto the biological subsystems discussed in Section~\ref{section_neuroscience}. The goal was to demonstrate how one might build systems that exhibit some desirable cognitive characteristics of human intelligence by leveraging ideas from neuroscience.

The PBWM ({\it{prefrontal cortex, basal ganglia, working memory}}) model described in~\cite{OReillyandFrankNC-06,HazyetalPTRS-07} covers territory that we only sample from, but the PBWM was developed to explain the function of biological brains, not selectively borrow ideas to extend the capabilities of artificial neural networks.

In a complete end-to-end architecture implementing the programmer's apprentice, the three subsystems described in this subsection would have to be integrated into different parts of the action selection and executive control systems: 
%
\begin{itemize}
%
%%% Figure 10: We use pointers to represent programs as abstract syntax trees ...
\item The system in Figure~{\urlh{#fig_Differentiable_Structured_Programs}{\ref{fig_programs}}} illustrates a differentiable procedural abstraction rich enough to support structured programming in a connectionist setting using standard embedding techniques and memory networks~\cite{WestonetalCoRR-14,DanihelkaetalCoRR-16,GravesetalCoRR-14,GravesetalNATURE-16}.
%
%%% Figure 11: This slide illustrates how we make use of input / output pairs as ...
\item The system in Figure~{\urlh{#fig_Differentiable_Program_Emulation}{\ref{fig_emulator}}} provides a sketch of how one might train a language model to predict the next statement in a program under construction using input / output pairs or other program invariants to constrain search~\cite{WangetalCoRR-18,WangetalCoRR-17,SinghandKohliSNAPL-17,DevlinetalICML-17}.
%
%%% Figure 12: The above graphic illustrates adapting imagination-based planning ...
\item The system in Figure~{\urlh{#Graph_Nets_Imagination_Coding}{\ref{fig_imagine}}} shows how a variation of imagination-based planning might be used to train a network to predict the next program state using the embodied integrated development environment as a source of ground truth~\cite{WeberetalCoRR-17,PascanuetalCoRR-17,HamricketalCoRR-17}.
%
\end{itemize}

Exactly how and where these components might be integrated into the overall architecture is beyond the scope of this paper, but research on the neural correlates of mathematical reasoning may provide some useful clues where to start~\cite{DehaeneetalCOGNITIVE-NEUROPSYCHOLOGY-03,DehaeneandBrannon2011mathminds,AmalricandDehaenePNAS-16}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END PROGRAMMING MODEL %%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% \input{./inputs/CONCLUSIONS.tex}

%%% File: ./inputs/CONCLUSIONS.tex

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Training the models described in this paper is a daunting challenge, especially when you consider that current deep neural network technologies rely heavily on large amounts of labeled data and applications like the programmer's apprentice are particularly vulnerable to catastrophic forgetting. We believe training will require radically new approaches and that cognitive and developmental neuroscience have much to offer in terms of insights drawn from the study of how humans learn. 

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Child Development}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

The human brain is organized as a 3-D structure in which specific cell types are positioned in a radial, laminar and areal arrangement that depends on the production, specialization and directed migration of cells from their origin in the embryo to their final destination~\cite{RakiketalTCN-09}. It is only on arriving at their final location that they establish connections to other cells. Postnatally laminar and areal differentiation exhibit substantial differences between early (2-3 months) and late (7-12 months) infancy~\cite{MolnaretalJoA-19,KostovicandJudasTCN-09}. Functional organization begins early (2-3 months) even as construction continues and the shaping of cortical circuits reflects the consequences of increasingly complex behavior. 

All of this carefully orchestrated activity is critical to development. Early brain structures appear as a consequence of the simple reflexive behaviors the infant engages in, laying the foundation for more coordinated behavior depending on increasingly complex internal representations. The infant's ability to engage its environment broadens, exposing it to more complicated stimuli and the opportunity to experiment with new behaviors. The physical and social environment seem to conspire to ensure that the growing infant and then adolescent has the necessary physical and intellectual prerequisites in place when exposed to circumstances that require them. It may be that we will find it useful to recapitulate some version of these developmental strategies for training architectures patterned after the human brain.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Inductive Bias}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Most AI systems are trained assuming what is essentially a blank slate in the form of random weights and objective functions that do little to influence the specific content of what is actually learned\footnote{%
%
  There are exceptions. For example, Ullman~\etal~\cite{UllmanetalPNAS-12} suggest a collection of innate biases that enable the infant visual system to learn to detect human hands by appearance and by context, as well as direction of gaze, in complex natural scenes; Raposo~\etal~\cite{RaposoetalCSC-17} develop models that learn the relational structure of objects and their various arrangements; and Battaglia~\etal~\cite{BattagliaetalCoRR-18} explore the idea of how introducing a relational inductive bias can expedite learning about entities, relations and the rules for their composition.}.
%
In contrast, babies are born with an innate understanding of how objects move around and interact with one another~\cite{Dehaene-LambertzandSpelkeNEURON-15}. Before they can even crawl about on their own, they appear to have an intuitive understanding of how space, time and number are interrelated~\cite{deHeviaetalPNAS-14,HauserandSpelkeTCN-04}. 
%
The bodies, neural architectures, physical and social environments of mammals provide a strong inductive bias in shaping their brains. Prolonged development plays a particularly important role in humans. Most mammals can stand and move about within hours of being born. Many human babies don't walk until just under one year. However, human infants learn a great deal during this early preperambulatory developmental period, much of it in preparation for subsequent stages of development~\cite{MacLeanPNAS-16,RosatietalEVOLUTIONARY-PSYCHOLOGY-14}.

It would be difficult if not impossible to genetically encode what a child learns during its lengthy development. And so it seems plausible that evolution would select for a compact general inductive bias enabling us to quickly acquire the basic skills we need to survive while retaining sufficient neural plasticity so that we can adapt to changes during our lifetimes. The schedule of developmental milestones necessary to learn these skills is highly conserved within our species and punctuated by profound changes in the architecture of the brain.

At birth, the architectural foundations are in place to construct the adult brain. For each subsequent developmental milestone, our genes turn on the specific cellular machinery necessary to construct scaffolding, guide neurons of the right cell types to their terminal locations, extend axonal and dendritic processes, eliminate unnecessary neurons and establish new or prune existing synaptic connections. The innate inductive bias and training curriculum implicit in development influence two critical factors that determine human intelligence: First, they serve to initialize the mapping from body to latent state representations throughout the cortex thereby grounding experience in the physical environment. Second, they utilize this grounding as the basis for all subsequent understanding, concrete and abstract. 

This basis provides a template or prototype\footnote{%
%
  Prototype theory is a mode of graded categorization in cognitive science, where some members of a category are more central than others. For example, when asked to give an example of the concept furniture, chair is more frequently cited than, say, stool. Prototype theory has also been applied in linguistics, as part of the mapping from phonological structure to semantics. ({\urlh{https://en.wikipedia.org/wiki/Prototype_theory}{SOURCE}})}
%
for representing new concepts, whether they be predictive models that allow us to interact with complex dynamical systems or composite categorical representations that enable us to recognize, contrast and compare instances of a particular class of entities~\cite{RoschNATURAL-CATEGORIES-95,RoschNATURAL-CATEGORIES-91,VarelaThompsonRoschTHE_EMBODIED_MIND-91}. 
%
In designing architectures to accommodate learning such representations, recent work on learning relational models that characterize different classes of entities, the relationships they participate in and the rules employed in composing them to form new relationships seems particularly promising~\cite{SanchezetalCoRR-18,HamricketalCoRR-18,SantoroetalNIPS-17,BattagliaetalNIPS-16}. This core competency should also serve as the starting point for reasoning about all sorts of abstract entities including computer programs and mathematical objects\footnote{%
%
  Learning entities, relations and their compositions may seem like it would require exotic new architectures, but the work of Hubel and Wiesel~\cite{HubelandWieselJoP-62,HubelandWieselJoP-61} published nearly sixty years ago that inspired Fukushima~\cite{FukushimaBC-80}, LeCun~\etal{}~\cite{LecunetalIEEE-98} and Riesenhuber and Poggio~\cite{RiesenhuberandPoggioNN-99} among many others is relevant to much more than extracting features from images.\\
  Convolutional neural networks are common in many applications areas including natural language processing~\cite{KimEMNLP-14}, medical data analysis~\cite{Luetal2017medical}, extreme weather condition prediction~\cite{LiuetalCoRR-16}, time series prediction for speech modeling~\cite{vandenOordCoRR-16} to name but a few. Many of these applications combine CNN and RNN technologies using CNN components to model coarse-grained local features generated and RNN models to account for long-distance dependencies~\cite{WangetalCOLING-16}.\\
  Convolutional layers in a multilayer stack employing a sparsity-inducing energy function essentially segment their input providing a basis for entity recognition. Subsequent pooling layers serve to identify correlations evident in earlier layers and thereby identify candidates for relationships, potentially including pairwise and higher-order relationships in a deep enough stack~\cite{SantoroetalNIPS-17}. In addition to Hubel and Wiesel, this technology owes a debt to Barlow~\cite{BarlowNC-89,Barlow61}, Rao and Ballard~\cite{RaoandBallardNATURE-NEUROSCIENCE-99} for foundational biologically-inspired work on sparse and predictive coding~\cite{ChalketalPNAS-18}.}.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Natural Language}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The use of language and, more generally, symbolic reasoning is an important if not defining characteristic of human intelligence. Some cognitive scientists, including such outspoken proponents as Jerry Fodor and Zenon Pylyshyn, view symbolic representations that exhibit combinatorial syntactic and semantic structure as candidates for a language of thought, and view connectionist proposals as lacking these properties and serving primarily as an account of the neural structures in which symbolic representations are implemented~\cite{FodorandPylyshynCOGNITION-88,Fodor84}. O'Reilly~\etal{}~\cite{OReillyetalTACO-14} have attempted to reconcile the symbolic and connectionist views, arguing that the two are complementary and that parts of the brain exhibit properties of both symbolic and connectionist information processing. 

The evolutionary biologist, Terrence Deacon, argues that our use of language is not a direct consequence of natural selection but rather the result of a collective effort involving millions of human beings working over thousands of year to produce an encyclopedic record of human endeavor to pass down to future generations~\cite{Deacon1998symbolic}. It is hard to imagine an effective programmer's apprentice, much less an accomplished software engineer, lacking the ability to communicate in natural language or denied access to the written word. Recent progress in grounding language learning in an agent's experience interacting with a suitably complex environment bodes well for applications like the programmer's apprentice. For these reasons and more, we see the rich complexity of collaborative pair programming as a compelling framework for exploring human-level AI.

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{100}

\bibitem{AimoneetalNEURON-09}
J.~B. Aimone, J.~Wiles, and F.~H. Gage.
\newblock Computational influence of adult neurogenesis on memory encoding.
\newblock {\em Neuron}, 61(2):187--202, 2009.

\bibitem{AimoneetalNEURON-11}
James~B. Aimone, Wei Deng, and Fred~H. Gage.
\newblock Resolving new memories: A critical look at the dentate gyrus, adult
  neurogenesis, and pattern separation.
\newblock {\em Neuron}, 70(4):589--596, 2011.

\bibitem{AmalricandDehaenePNAS-16}
Marie Amalric and Stanislas Dehaene.
\newblock Origins of the brain networks for advanced mathematics in expert
  mathematicians.
\newblock {\em Proceedings of the National Academy of Sciences},
  113(18):4909--4917, 2016.

\bibitem{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock {\em arXiv preprint arXiv:1606.06565}, 2016.

\bibitem{AndreasetalICML-17}
Jacob Andreas, Dan Klein, and Sergey Levine.
\newblock Modular multitask reinforcement learning with policy sketches.
\newblock {\em CoRR}, arXiv:1611.01796, 2016.

\bibitem{AndrychowiczetalCoRR-17}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba.
\newblock Hindsight experience replay.
\newblock {\em CoRR}, arXiv:1707.01495, 2017.

\bibitem{SpelkeNEW-YORK-TIMES-12}
Natalie Angier.
\newblock {Insights From the Youngest Minds}.
\newblock Feature article on Elizabeth Spelke in the New York Times, April 30,
  2012, 2012.

\bibitem{AnsetalCSS-02}
Bernard Ans, St\`{e}phane Rousset, Robert French, and Serban Musca.
\newblock Preventing catastrophic interference in multiple-sequence learning
  using coupled reverberating elman networks.
\newblock {\em Proceedings of the 24th Annual Meeting of the Cognitive Science
  Society}, 2002.

\bibitem{ArdeschetalPNAS-19}
Dirk~Jan Ardesch, Lianne~H. Scholtens, Longchuan Li, Todd~M. Preuss, James~K.
  Rilling, and Martijn~P. van~den Heuvel.
\newblock Evolutionary expansion of connectivity between multimodal association
  areas in the human brain compared with chimpanzees.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(14):7101--7106, 2019.

\bibitem{Arenkiel2014neural}
B.R. Arenkiel.
\newblock {\em Neural Tracing Methods: Tracing Neurons and Their Connections}.
\newblock Springer New York, 2014.

\bibitem{Baars1988}
B.~J. Baars.
\newblock {\em A cognitive theory of consciousness}.
\newblock Cambridge University Press, New York, NY, 1988.

\bibitem{BaddeleyQJoEP-86}
Alan Baddeley.
\newblock Modularity, mass-action and memory.
\newblock {\em The Quarterly Journal of Experimental Psychology Section A},
  38(4):527--533, 1986.

\bibitem{BadreandWagnerNEURON-04}
D.~Badre and A.~D. Wagner.
\newblock Selection, integration, and conflict monitoring; assessing the nature
  and generality of prefrontal cognitive control mechanisms.
\newblock {\em Neuron}, 41(3):473--487, 2004.

\bibitem{BadreTiCS-08}
David Badre.
\newblock Cognitive control, hierarchy, and the rostro–caudal organization of
  the frontal lobes.
\newblock {\em Trends in Cognitive Sciences}, 12(5):193--200, 2008.

\bibitem{BakkerandSchmidhuberIAS-04}
Bram Bakker and J\"{u}rgen Schmidhuber.
\newblock Hierarchical reinforcement learning based on subgoal discovery and
  subpolicy specialization.
\newblock In {\em Proceedings of the 8th Conference on Intelligent Autonomous
  Systems}, pages 438--445, 2004.

\bibitem{BarlowNC-89}
Horace~B. Barlow.
\newblock Unsupervised learning.
\newblock {\em Neural Computation}, 1:295--311, 1989.

\bibitem{BattagliaetalNIPS-16}
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo~Jimenez Rezende, and Koray
  Kavukcuoglu.
\newblock Interaction networks for learning about objects, relations and
  physics.
\newblock In {\em Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 4509--4517. Curran Associates Inc.,
  2016.

\bibitem{BattagliaetalCoRR-18}
Peter~W. Battaglia, Jessica~B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
  Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam
  Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin
  Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria
  Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt
  Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu.
\newblock Relational inductive biases, deep learning, and graph networks.
\newblock {\em CoRR}, arXiv:1806.01261, 2018.

\bibitem{BeckerHIPPOCAMPUS-05}
S.~Becker.
\newblock A computational principle for hippocampal learning and neurogenesis.
\newblock {\em Hippocampus}, 15(6):722--738, 2005.

\bibitem{belilovsky2018greedy}
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon.
\newblock Greedy layerwise learning can scale to imagenet.
\newblock {\em arXiv preprint arXiv:1812.11446}, 2018.

\bibitem{BendorandWilsonNATURE-NEUROSCIENCE-12}
Daniel Bendor and Matthew~A Wilson.
\newblock Biasing the content of hippocampal replay during sleep.
\newblock {\em Nature neuroscience}, 15(10):1439, 2012.

\bibitem{BengioetalCoRR-15}
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer.
\newblock Scheduled sampling for sequence prediction with recurrent neural
  networks.
\newblock {\em CoRR}, arXiv:1506.03099, 2015.

\bibitem{BengioCoRR-17}
Yoshua Bengio.
\newblock The consciousness prior.
\newblock {\em CoRR}, arXiv:1709.08568, 2017.

\bibitem{BengioetalNIPS-07}
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.
\newblock Greedy layer-wise training of deep networks.
\newblock In {\em Advances in Neural Information Processing Systems 19}, pages
  153--160. MIT Press, Cambridge, MA, 2007.

\bibitem{BengioetalICML-09}
Yoshua Bengio, J{\'e}r\^{o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 41--48, New York, NY, USA, 2009. ACM.

\bibitem{BinderandDesaiTiCS-11}
J.~R. Binder and R.~H. Desai.
\newblock The neurobiology of semantic memory.
\newblock {\em Trends in Cognitive Science}, 15(11):527--536, 2011.

\bibitem{BlouwandEliasmithCSS-13}
Peter Blouw and Chris Eliasmith.
\newblock A neurally plausible encoding of word order information into a
  semantic vector space.
\newblock In {\em 35th Annual Conference of the Cognitive Science Society},
  pages 1905--1910, 2013.

\bibitem{BotvinickandAnANIPS-09}
Matthew Botvinick and James An.
\newblock Goal-directed decision making in prefrontal cortex: A computational
  framework.
\newblock {\em Advances in Neural Information Processing Systems}, 21:169--176,
  2009.

\bibitem{BotvinickPTRS_B-07}
Matthew~M. Botvinick.
\newblock Multilevel structure in behaviour and in the brain: a model of
  fuster's hierarchy.
\newblock {\em Philosophical transactions of the Royal Society of London.
  Series B, Biological sciences}, 362:1615--1626, 2007.

\bibitem{BoydenBIOLOGY-11}
Edward Boyden.
\newblock A history of optogenetics: the development of tools for controlling
  brain circuits with light.
\newblock {\em F1000 Biology Reports}, 3, 2011.

\bibitem{BridgeandClarePTRS-B-06}
Holly Bridge and Stuart Clare.
\newblock High-resolution {MRI}: {\em{in vivo}} histology?
\newblock {\em Philosophical transactions of the Royal Society of London.
  Series B, Biological sciences}, 361:137--146, 2006.

\bibitem{Brodmann1909}
Korbinian Brodmann.
\newblock {\em {Vergleichende Lokalisationslehre der Grosshirnrinde in ihren
  Prinzipien dargestellt auf Grund des Zellenbaues}}.
\newblock Johann Ambrosius Barth Verlag, Leipzig, 1909.

\bibitem{BuxtonetalNEUROIMAGING-04}
R.B. Buxton, K.~Uludag, D.J. Dubowitz, and T.T. Liu.
\newblock Modeling the hemodynamic response to brain activation.
\newblock {\em Neuroimaging}, 23:220--233, 2004.

\bibitem{CallawayCURRENT-OPINION-08}
Edward~M. Callaway.
\newblock Transneuronal circuit tracing with neurotropic viruses.
\newblock {\em Current opinion in neurobiology}, 18:617--623, 2008.

\bibitem{CarretalNATURE-NEUROSCIENCE-11}
Margaret~F Carr, Shantanu~P Jadhav, and Loren~M Frank.
\newblock Hippocampal replay in the awake state: a potential substrate for
  memory consolidation and retrieval.
\newblock {\em Nature neuroscience}, 14(2):147, 2011.

\bibitem{ChenetalNEURON-18}
X.~Chen, Y.~Mu, Y.~Hu, A.~T. Kuan, M.~Nikitchenko, O.~Randlett, A.~B. Chen,
  J.~P. Gavornik, H.~Sompolinsky, F.~Engert, and M.~B. Ahrens.
\newblock {Brain-wide Organization of Neuronal Activity and Convergent
  Sensorimotor Transformations in Larval Zebrafish}.
\newblock {\em Neuron}, 100(4):876--890, 2018.

\bibitem{ZhiyuanandBingLML-18}
Zhiyuan Chen and Bing Liu.
\newblock Continual learning and catastrophic forgetting.
\newblock In {\em Lifelong Machine Learning, Second Edition}, volume~12 of {\em
  Synthesis Lectures on Artificial Intelligence and Machine Learning}, pages
  1--207. Morgan \& Claypool Publishers, 2018.

\bibitem{WangetalCoRR-18}
Alex Polozov Marc Brockschmidt Rishabh~Singh Chenglong~Wang, Po-Sen~Huang.
\newblock Execution-guided neural program decoding.
\newblock {\em CoRR}, arXiv:1807.03100, 2018.

\bibitem{ChicaetalBSF-18}
Ana~B. Chica, Michel Thiebaut~de Schotten, Paolo Bartolomeo, and Pedro~M.
  Paz-Alonso.
\newblock White matter microstructure of attentional networks predicts
  attention and consciousness functional interactions.
\newblock {\em Brain Structure and Function}, 223(2):653--668, 2018.

\bibitem{ClarkTiCS-04}
Eve~V. Clark.
\newblock How language acquisition builds on cognitive development.
\newblock {\em Trends in Cognitive Sciences}, 8(10):472--478, 2004.

\bibitem{NealandEichenbaum1993}
Neal~J. Cohen and Howard Eichenbaum.
\newblock {\em Memory, amnesia, and the hippocampal system.}
\newblock The MIT Press, Cambridge, MA, US, 1993.

\bibitem{CourtneyCABN-04}
S.~M. Courtney.
\newblock Attention and cognitive control as emergent properties of information
  representation in working memory.
\newblock {\em Cognitive, Affective and Behavioral Neuroscience},
  4(4):501--516, 2004.

\bibitem{CowanPBR-08}
Nelson Cowan.
\newblock What are the differences between long-term, short-term, and working
  memory?
\newblock {\em Progress in Brain Research}, 169:323--338, 2008.

\bibitem{DanihelkaetalCoRR-16}
Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves.
\newblock Associative long short-term memory.
\newblock {\em CoRR}, arXiv:1602.03032, 2016.

\bibitem{deHeviaetalPNAS-14}
Maria~Dolores de~Hevia, V\'{e}ronique Izard, Aur\'{e}lie Coubart, Elizabeth~S.
  Spelke, and Arlette Streri.
\newblock Representations of space, time, and number in neonates.
\newblock {\em Proceedings of the National Academy of Sciences},
  111(13):4809--4813, 2014.

\bibitem{Deacon1998symbolic}
Terrence~W. Deacon.
\newblock {\em The Symbolic Species: The Co-evolution of Language and the
  Brain}.
\newblock W. W. Norton, 1998.

\bibitem{DeakerBCN-14}
M.~O. De\'{a}k.
\newblock Interrelations of language and cognitive development.
\newblock In P.~Brooks \&~V. Kampe, editor, {\em The International Encyclopedia
  of the Social \& Behavioral Sciences}, pages 284--291. SAGE, 2014.

\bibitem{DehaeneetalCOGNITIVE-NEUROPSYCHOLOGY-03}
S.~Dehaene, M.~Piazza, P.~Pinel, and L.~Cohen.
\newblock {T}hree parietal circuits for number processing.
\newblock {\em Cognitive Neuropsychology}, 20(3):487--506, 2003.

\bibitem{Dehaene2014}
Stanislas Dehaene.
\newblock {\em Consciousness and the Brain: Deciphering How the Brain Codes Our
  Thoughts}.
\newblock Viking Press, 2014.

\bibitem{DehaeneandBrannon2011mathminds}
Stanislas. Dehaene and Elizabeth Brannon.
\newblock {\em Space, Time and Number in the Brain: Searching for the
  Foundations of Mathematical Thought}.
\newblock Elsevier Science, 2011.

\bibitem{Dehaene-LambertzandSpelkeNEURON-15}
G.~Dehaene-Lambertz and E.S. Spelke.
\newblock The infancy of the human brain.
\newblock {\em Neuron}, 88(1):93--109, 2015.

\bibitem{DEspositoetalNATURE-95}
M.~D'Esposito, J.~A. Detre, D.~C. Alsop, R.~K. Shin, S.~Atlas, and M.~Grossman.
\newblock The neural basis of the central executive system of working memory.
\newblock {\em Nature}, 378(6554):279--281, 1995.

\bibitem{DevlinetalICML-17}
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman
  Mohamed, and Pushmeet Kohli.
\newblock Robustfill: Neural program learning under noisy i/o.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, pages 990--998. JMLR.org, 2017.

\bibitem{DietterichJAIR-00}
Thomas~G. Dietterich.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function
  decomposition.
\newblock {\em Journal of Artificial Intelligence Research}, 13:227--303, 2000.

\bibitem{DiuketalCRMHOB-13}
C.~Diuk, A.~Schapiro, N.~C\'{o}rdova, J.~Ribas-Fernandes, Y.~Niv, and
  M.~Botvinick.
\newblock Divide and conquer: hierarchical reinforcement learning and task
  decomposition in humans.
\newblock In {\em Computational and robotic models of the hierarchical
  organization of behavior}, pages 271--291, Berlin, Heidelberg, 2013.
  Springer.

\bibitem{DombeckandTankCSH-11}
Daniel Dombeck and David Tank.
\newblock Imaging in neuroscience.
\newblock In Helmchen and Konnerth, editors, {\em Two-Photon Imaging of Neural
  Activity in Awake Mobile Mice}, pages 355--369. Cold Spring Harbor Press,
  2011.

\bibitem{DouglasandMartinCURRENT-BIOLOGY-12}
Rodney~J. Douglas and Kevan~A.C. Martin.
\newblock Behavioral architecture of the cortical sheet.
\newblock {\em Current Biology}, 22(24):R1033--R1038, 2012.

\bibitem{DrewetalLEARNING-MEMORY-13}
L.~J. Drew, S.~Fusi, and R.~Hen.
\newblock Adult neurogenesis in the mammalian hippocampus: {W}hy the dentate
  gyrus?
\newblock {\em Learning and Memory}, 20(12):710--729, 2013.

\bibitem{EisenbergetalSCIENCE-03}
Mark Eisenberg, Tali Kobilo, Diego~E. Berman, and Yadin Dudai.
\newblock Stability of retrieved memory: Inverse correlation with trace
  dominance.
\newblock {\em Science}, 301(5636):1102--1104, 2003.

\bibitem{Eliasmith2013}
Chris Eliasmith.
\newblock {\em How to Build a Brain: A Neural Architecture for Biological
  Cognition}.
\newblock Oxford Series on Cognitive Modeling. Oxford University Press {USA},
  2013.

\bibitem{Fodor84}
Jerry Fodor.
\newblock {\em Modularity of Mind}.
\newblock {MIT} Press, Cambridge, Massachusetts, 1984.

\bibitem{FodorandPylyshynCOGNITION-88}
Jerry~A. Fodor and Zenon~W. Pylyshyn.
\newblock Connectionism and cognitive architecture.
\newblock {\em Cognition}, 28(1-2):3--71, 1988.

\bibitem{FosterandWilsonNATURE-06}
David~J Foster and Matthew~A Wilson.
\newblock Reverse replay of behavioural sequences in hippocampal place cells
  during the awake state.
\newblock {\em Nature}, 440(7084):680, 2006.

\bibitem{FrankandBadreCEREBRAL-CORTEX-12}
M.~J. Frank and D.~Badre.
\newblock Mechanisms of hierarchical reinforcement learning in corticostriatal
  circuits {I}: computational analysis.
\newblock {\em Cerebral Cortex}, 22(3):509--526, 2012.

\bibitem{FrenchTiCS-99}
Robert French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock {\em Trends in Cognitive Sciences}, 3:128--135, 1999.

\bibitem{FrenchCONNECTION-SCIENCE-97}
Robert~M. French.
\newblock Pseudo-recurrent connectionist networks: An approach to the
  'sensitivity-stability' dilemma.
\newblock {\em Connection Science}, 9(4):353--380, 1997.

\bibitem{FukushimaBC-80}
K.~Fukushima.
\newblock Neocognitron: {A} self organizing neural network model for a
  mechanism of pattern recognition unaffected by shift in position.
\newblock {\em Biological Cybernnetics}, 36:93--202, 1980.

\bibitem{FusterPREFRONTAL-CORTEX-15-CHAPTER_8}
Joaqu\'{i}n~M. Fuster.
\newblock {\em {Chapter 8: An Overview of Prefrontal Functions}}, pages
  375--425.
\newblock Elsevier, London, 2015.

\bibitem{FusterPREFRONTAL-CORTEX-15}
Joaqu\'{i}n~M. Fuster.
\newblock {\em {Prefrontal Cortex, 5th Edition}}.
\newblock Elsevier, London, 2015.

\bibitem{GeetalNATURE-06}
S.~Ge, E.~L. Goh, K.~A. Sailor, Y.~Kitabatake, G.~L. Ming, and H.~Song.
\newblock {GABA} regulates synaptic integration of newly generated neurons in
  the adult brain.
\newblock {\em Nature}, 439(7076):589--593, 2006.

\bibitem{GershmanandDawANNUAL-REVIEWS-17}
Samuel~J. Gershman and Nathaniel~D. Daw.
\newblock Reinforcement learning and episodic memory in humans and animals:
  {A}n integrative framework.
\newblock {\em Annual Reviews of Psychology}, 68:101–128, 2017.

\bibitem{GibsonPERCEPTION-50}
James~J. Gibson.
\newblock {\em Perception of the Visual World}.
\newblock Houghton Mifflin, Boston, 1950.

\bibitem{GoenseetalFiCN-16}
Jozien Glense, Yvette Bohraus, and Nikos~K. Logothetis.
\newblock {fMRI} at high spatial resolution: Implications for {BOLD}-models.
\newblock {\em Frontiers in computational neuroscience}, 10:66--66, 2016.

\bibitem{GloverPMC-11}
Gary~H. Glover.
\newblock Overview of functional magnetic resonance imaging.
\newblock {\em Neurosurgery clinics of North America}, 22:133--144, 2011.

\bibitem{Goldman-RakicARN-88}
Patricia~S. Goldman-Rakic.
\newblock Topography of cognition: Parallel distributed networks in primate
  association cortex.
\newblock {\em Annual Review of Neuroscience}, 11(1):137--156, 1988.

\bibitem{Gomez-RoblesetalPNAS-15}
Aida G{\'o}mez-Robles, William~D. Hopkins, Steven~J. Schapiro, and Chet~C.
  Sherwood.
\newblock Relaxed genetic control of cortical organization in human brains
  compared with chimpanzees.
\newblock {\em Proceedings of the National Academy of Sciences},
  112(48):14799--14804, 2015.

\bibitem{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em 2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 6645--6649. IEEE, 2013.

\bibitem{GravesetalCoRR-14}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural {T}uring machines.
\newblock {\em CoRR}, arXiv:1410.5401, 2014.

\bibitem{GravesetalNATURE-16}
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka
  Grabska-Barwi\'{n}ska, Sergio~G\'{o}mez Colmenarejo, Edward Grefenstette,
  Tiago Ramalho, John Agapiou, Adri\`{a}~Puigdom\'{e}nech Badia, Karl~Moritz
  Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher
  Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock {\em Nature}, 538:471--476, 2016.

\bibitem{GregoretalCoRR-15}
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra.
\newblock {DRAW}: A recurrent neural network for image generation.
\newblock {\em CoRR}, arXiv:1502.04623, 2015.

\bibitem{GuezetalCoRR-18}
Arthur Guez, Th{\'{e}}ophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol
  Vinyals, Daan Wierstra, R{\'{e}}mi Munos, and David Silver.
\newblock Learning to search with {MCTS}nets.
\newblock {\em CoRR}, arXiv:1802.04697, 2018.

\bibitem{guillery2005postnatal}
RW~Guillery.
\newblock Is postnatal neocortical maturation hierarchical?
\newblock {\em Trends in neurosciences}, 28(10):512--517, 2005.

\bibitem{GulcehreetalCoRR-16}
Caglar G\"{u}lcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio.
\newblock Mollifying networks.
\newblock {\em CoRR}, arXiv:1608.04980, 2016.

\bibitem{HamricketalCoRR-18}
Jessica~B. Hamrick, Kelsey~R. Allen, Victor Bapst, Tina Zhu, Kevin~R. McKee,
  Joshua~B. Tenenbaum, and Peter~W. Battaglia.
\newblock Relational inductive bias for physical construction in humans and
  machines.
\newblock {\em CoRR}, abs/1806.01203, 2018.

\bibitem{HamricketalCoRR-17}
Jessica~B. Hamrick, Andrew~J. Ballard, Razvan Pascanu, Oriol Vinyals, Nicolas
  Heess, and Peter~W. Battaglia.
\newblock Metacontrol for adaptive imagination-based optimization.
\newblock {\em CoRR}, arXiv:1705.02670, 2017.

\bibitem{HanetalNATURE-18}
Yunyun Han, Justus~M. Kebschull, Robert A.~A. Campbell, Devon Cowan, Fabia
  Imhof, Anthony~M. Zador, and Thomas~D. Mrsic-Flogel.
\newblock The logic of single-cell projections from visual cortex.
\newblock {\em Nature}, 556:51--56, 2018.

\bibitem{HauserandSpelkeTCN-04}
Marc~D. Hauser and Elizabeth Spelke.
\newblock Evolutionary and developmental foundations of human knowledge: A case
  study of mathematics.
\newblock In M.~Gazzaniga and N.~Logothetis, editors, {\em The Cognitive
  Neurosciences, {III}}, pages 853--864. MIT Press, Cambridge, MA, 2004.

\bibitem{HawrylyczetalNATURE-NEUROSCIENCE-15}
Michael Hawrylycz, Jeremy~A. Miller, Vilas Menon, David Feng, Tim Dolbeare,
  Angela~L. Guillozet-Bongaarts, Anil~G. Jegga, Bruce~J. Aronow, Chang-Kyu Lee,
  Amy Bernard, Matthew~F. Glasser, Donna~L. Dierker, J\"{o}rg Menche, Aaron
  Szafer, Forrest Collman, Pascal Grange, Kenneth~A. Berman, Stefan Mihalas,
  Zizhen Yao, Lance Stewart, Albert-L\'{a}szl\'{o} Barab\'{a}si, Jay Schulkin,
  John Phillips, Lydia Ng, Chinh Dang, David~R. Haynor, Allan Jones, David~C.
  Van~Essen, Christof Koch, and Ed~Lein.
\newblock Canonical genetic signatures of the adult human brain.
\newblock {\em Nature Neuroscience}, 18:1832--1844, 2015.

\bibitem{HawrylyczetalNATURE-12}
Michael~J. Hawrylycz, Ed~S. Lein, Angela~L. Guillozet-Bongaarts, Elaine~H.
  Shen, Lydia Ng, Jeremy~A. Miller, Louie~N. van~de Lagemaat, Kimberly~A.
  Smith, Amanda Ebbert, Zackery~L. Riley, Chris Abajian, Christian~F. Beckmann,
  Amy Bernard, Darren Bertagnolli, Andrew~F. Boe, Preston~M. Cartagena,
  M.~Mallar Chakravarty, Mike Chapin, Jimmy Chong, Rachel~A. Dalley,
  Barry~David Daly, Chinh Dang, Suvro Datta, Nick Dee, Tim~A. Dolbeare, Vance
  Faber, David Feng, David~R. Fowler, Jeff Goldy, Benjamin~W. Gregor, Zeb
  Haradon, David~R. Haynor, John~G. Hohmann, Steve Horvath, Robert~E. Howard,
  Andreas Jeromin, Jayson~M. Jochim, Marty Kinnunen, Christopher Lau, Evan~T.
  Lazarz, Changkyu Lee, Tracy~A. Lemon, Ling Li, Yang Li, John~A. Morris,
  Caroline~C. Overly, Patrick~D. Parker, Sheana~E. Parry, Melissa Reding,
  Joshua~J. Royall, Jay Schulkin, Pedro~Adolfo Sequeira, Clifford~R.
  Slaughterbeck, Simon~C. Smith, Andy~J. Sodt, Susan~M. Sunkin, Beryl~E.
  Swanson, Marquis~P. Vawter, Derric Williams, Paul Wohnoutka, H.~Ronald
  Zielke, Daniel~H. Geschwind, Patrick~R. Hof, Stephen~M. Smith, Christof Koch,
  Seth G.~N. Grant, and Allan~R. Jones.
\newblock An anatomically comprehensive atlas of the adult human brain
  transcriptome.
\newblock {\em Nature}, 489:391--399, 2012.

\bibitem{HazyetalPTRS-07}
T.~E. Hazy, M.~J. Frank, and R.~C. O'reilly.
\newblock Towards an executive without a homunculus: computational models of
  the prefrontal cortex/basal ganglia system.
\newblock {\em Philosophical Transactions of the Royal Society London B,
  Biological Science}, 362(1485):1601--1613, 2007.

\bibitem{HengstEMLDM-17}
Bernhard Hengst.
\newblock Hierarchical reinforcement learning.
\newblock In Claude Sammut and Geoffrey~I. Webb, editors, {\em Encyclopedia of
  Machine Learning and Data Mining}, pages 611--619. Springer US, Boston, MA,
  2017.

\bibitem{HochreiterandSchmidhuberNC-97}
Sepp Hochreiter and J\"{u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Computing}, 9:1735--1780, 1997.

\bibitem{Jackson1958selected}
John~Hughlings Jackson.
\newblock {\em Selected Writings of John Hughlings Jackson: Evolution and
  dissolution of the nervous system}.
\newblock Selected Writings of John Hughlings Jackson. Basic Books, 1958.

\bibitem{JanuszewskietalNATURE-METHODS-18}
Michal Januszewski, J\"{o}rgen Kornfeld, Peter~H Li, Art Pope, Tim Blakely,
  Larry Lindsey, Jeremy~B Maitin-Shepard, Mike Tyka, Winfried Denk, and Viren
  Jain.
\newblock High-precision automated reconstruction of neurons with flood-filling
  networks.
\newblock {\em Nature Methods}, 15:605--610, 2017.

\bibitem{JilketalJETAI-08}
David Jilk, Christian Lebiere, Randall O'Reilly, and John R.~Anderson.
\newblock {SAL}: An explicitly pluralistic cognitive architecture.
\newblock {\em Journal Experimental and Theoretical Artificial Intelligence},
  20:197--218, 2008.

\bibitem{JooandFrankNATURE-REVIEWS-NEUROSCIENCE-18}
Hannah~R. Joo and Loren~M. Frank.
\newblock The hippocampal sharp wave-ripple in memory retrieval for immediate
  use and consolidation.
\newblock {\em Nature Reviews Neuroscience}, 19:744--757, 2018.

\bibitem{KaelblingICML-93}
Leslie~Pack Kaelbling.
\newblock Hierarchical reinforcement learning: A preliminary report.
\newblock In {\em Proceedings Tenth International Conference on Machine
  Learning}, pages 167--173, 1993.

\bibitem{KarlssonandFrankNATURE-NEUROSCIENCE-09}
Mattias~P Karlsson and Loren~M Frank.
\newblock Awake replay of remote experiences in the hippocampus.
\newblock {\em Nature neuroscience}, 12(7):913, 2009.

\bibitem{KesnerandRollsNBR-15}
R.~P. Kesner and E.~T. Rolls.
\newblock A computational theory of hippocampal function, and tests of the
  theory: {N}ew developments.

\bibitem{KirkpatricketalCoRR-16}
James Kirkpatrick, Razvan Pascanu, Neil~C. Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska{-}Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em CoRR}, arXiv:1612.00796, 2016.

\bibitem{KitabatakeetalNCNM-07}
Y.~Kitabatake, K.~A. Sailor, G.~L. Ming, and H.~Song.
\newblock Adult neurogenesis and hippocampal memory function: new cells, more
  plasticity, new memories?
\newblock {\em Neurosurgery Clinics of North America}, 18(1):105--113, 2007.

\bibitem{KoechlinetalSCIENCE-03}
Etienne Koechlin, Chryst\`{e}le Ody, and Fr\'{e}d\'{e}rique Kouneiher.
\newblock The architecture of cognitive control in the human prefrontal cortex.
\newblock {\em Science}, 302:1181--1185, 2003.

\bibitem{KolsteretalJoN-09}
H.~Kolster, J.~B. Mandeville, J.~T. Arsenault, L.~B. Ekstrom, L.~L. Wald, and
  W.~Vanduffel.
\newblock Visual field map clusters in macaque extrastriate visual cortex.
\newblock {\em Journal Neuroscience}, 29(21):7031--7039, 2009.

\bibitem{KonkleandCaramazzaJoN-13}
Talia Konkle and Alfonso Caramazza.
\newblock Tripartite organization of the ventral stream by animacy and object
  size.
\newblock {\em Journal of Neuroscience}, 33(25):10235--10242, 2013.

\bibitem{KostovicandJudasTCN-09}
Ivica Kostovi\'{c} and Milo\v{s} Juda\v{s}.
\newblock Early development of neuronal circuitry of the human prefrontal
  cortex.
\newblock In Michael~S. Gazzaniga, editor, {\em The Cognitive Neurosciences,
  4th Edition}, pages 29--48. The MIT Press, Cambridge, MA, 2009.

\bibitem{KozioletalCEREBELLUM-14}
Leonard~F. Koziol, Deborah Budding, Nancy Andreasen, Stefano D'Arrigo, Sara
  Bulgheroni, Hiroshi Imamizu, Masao Ito, Mario Manto, Cherie Marvel, Krystal
  Parker, Giovanni Pezzulo, Narender Ramnani, Daria Riva, Jeremy Schmahmann,
  Larry Vandervert, and Tadashi Yamazaki.
\newblock Consensus paper: the cerebellum's role in movement and cognition.
\newblock {\em Cerebellum (London, England)}, 13:151--177, 2014.

\bibitem{KulkarnietalNIPS-16}
Tejas~D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock In {\em Advances in Neural Information Processing Systems 29}, pages
  3675--3683, 2016.

\bibitem{KumaranetalTiCS-16}
Dharshan Kumaran, Demis Hassabis, and James~L. McClelland.
\newblock What learning systems do intelligent agents need? {C}omplementary
  learning systems theory updated.
\newblock {\em Trends in Cognitive Sciences}, 20(7):512--534, 2016.

\bibitem{KumaranandMaguireJoN-05}
Dharshan Kumaran and Eleanor~A. Maguire.
\newblock The human hippocampus: Cognitive maps or relational memory?
\newblock {\em Journal of Neuroscience}, 25(31):7254--7259, 2005.

\bibitem{LebiereandAndersonCSS-93}
Christian Lebiere and John Anderson.
\newblock A connectionist implementation of the act-r production system.
\newblock In {\em Proceedings of the Fifteenth Annual Conference of the
  Cognitive Science Society}, pages 635--640. Cognitive Science Society, 1993.

\bibitem{Lettvinetal59}
J.~Y. Lettvin, H.~R. Maturana, W.~S. McCulloch, and W.~H. Pitts.
\newblock What the frog's eye tells the frog's brain.
\newblock {\em Proceedings of the Institute for Radio Engineers},
  47:1940--1951, 1959.

\bibitem{LinML-92}
Long-Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock {\em Machine Learning}, 8(3):293--321, 1992.

\bibitem{LittmanetalUAI-95}
Michael Littman, Thomas Dean, and Leslie Kaelbling.
\newblock On the complexity of solving {M}arkov decision problems.
\newblock In {\em Proceedings of the 11th Conference on Uncertainty in
  Artificial Intelligence}, pages 394--402, San Francisco, California, 1995.
  AUAI, Morgan Kaufmann Publishers.

\bibitem{LongetalPNAS-18}
Bria Long, Chen-Ping Yu, and Talia Konkle.
\newblock Mid-level visual features underlie the high-level categorical
  organization of the ventral stream.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(38):E9015--E9024, 2018.

\bibitem{MacLeanPNAS-16}
E.~L. MacLean.
\newblock Unraveling the evolution of uniquely human cognition.
\newblock {\em Proceedings of the National Academy of Sciences},
  113(23):6348--6354, 2016.

\bibitem{MarrandBrindleyPTRS_B-71}
D.~Marr and Giles~Skey Brindley.
\newblock Simple memory: a theory for archicortex.
\newblock {\em Philosophical Transactions of the Royal Society of London. B,
  Biological Sciences}, 262(841):23--81, 1971.

\bibitem{McClelland79}
J.~McClelland.
\newblock On the time relations of mental processes: {A}n examination of
  systems of processes in cascade.
\newblock {\em Psychological Review}, 86:287--330, 1979.

\bibitem{McClellandandRumelhartPR-88}
J.~McClelland and D.~Rumelhart.
\newblock An interactive activation model of context effects in letter
  perception: {I}. {A}n account of basic findings.
\newblock {\em Psychological Review}, 88:375--407, 1981.

\bibitem{McClellandandGoddardHIPPOCAMPUS-97}
J.~L. McClelland and N.~H. Goddard.
\newblock Considerations arising from a complementary learning systems
  perspective on hippocampus and neocortex.
\newblock {\em Hippocampus}, 6(6):654--665, 1996.

\bibitem{McClellandetalPR-95}
James~L. McClelland, Bruce~L. McNaughton, and Randall~C. O'Reilly.
\newblock Why there are complementary learning systems in the hippocampus and
  neocortex: {I}nsights from the successes and failures of connectionist models
  of learning and memory.
\newblock {\em Psychological Review}, 102(3):419--457, 1995.

\bibitem{mccloskey2011corollary}
DI~McCloskey.
\newblock Corollary discharges: motor commands and perception.
\newblock {\em comprehensive physiology}, pages 1415--1447, 2011.

\bibitem{McCullochandPitts43}
W.~S. McCulloch and W.~H. Pitts.
\newblock A logical calculus of ideas immanent in nervous activity.
\newblock {\em Bulletin of Mathematical Biophysics}, 5:115--133, 1943.

\bibitem{MenegasetalELIFE-17}
William Menegas, Benedicte~M Babayan, Naoshige Uchida, and Mitsuko
  Watabe-Uchida.
\newblock Opposite initialization to novel cues in dopamine signaling in
  ventral and posterior striatum in mice.
\newblock {\em Elife}, 6:e21886, 2017.

\bibitem{MikulaandDenkNATURE-METHODS-15}
Shawn Mikula and Winfried Denk.
\newblock High-resolution whole-brain staining for electron microscopic circuit
  reconstruction.
\newblock {\em Nature Methods}, 2015.

\bibitem{MnihetalCoRR-16}
Volodymyr Mnih, Adri{\`{a}}~Puigdom{\`{e}}nech Badia, Mehdi Mirza, Alex Graves,
  Timothy~P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock {\em CoRR}, arXiv:1602.01783, 2016.

\bibitem{MnihetalCoRR-13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing {Atari} with deep reinforcement learning.
\newblock {\em CoRR}, arXiv:1312.5602, 2013.

\bibitem{MoffaertandNoweJMLR-14}
Kristof~Van Moffaert and Ann Now\'{e}.
\newblock Multi-objective reinforcement learning using sets of pareto
  dominating policies.
\newblock {\em Journal of Machine Learning Research}, 15:3663--3692, 2014.

\bibitem{MolnaretalJoA-19}
Zolt\'{a}n Moln\'{a}r, Gavin~J. Clowry, Nenad Sestan, Ayman Alzu'bi, Trygve
  Bakken, Robert~F. Hevner, Petra~S. H\"{u}ppi, Ivica Kostovi\'{c}, Pasko
  Rakic, E.~S. Anton, David Edwards, Patricia Garcez, Anna Hoerder-Suabedissen,
  and Arnold Kriegstein.
\newblock New insights into the development of the human cerebral cortex.
\newblock {\em Journal of Anatomy}, 235(3):432--451, 2019.

\bibitem{Mora-BermudezetalELIFE-16}
Felipe Mora-Berm{\'{u}}dez, Farhath Badsha, Sabina Kanton, J.~Gray Camp,
  Benjamin Vernot, Kathrin K{\"{o}}hler, Birger Voigt, Keisuke Okita, Tomislav
  Maricic, Zhisong He, Robert Lachmann, Svante Paabo, Barbara Treutlein, and
  Wieland~B. Huttner.
\newblock Differences and similarities between human and chimpanzee neural
  progenitors during cerebral cortex development.
\newblock {\em eLife}, 5:e18683, 2016.

\bibitem{MorrisetalNEURON-06}
Richard~G.M. Morris, Jennifer Inglis, James~A. Ainge, Henry~J. Olverman, Jane
  Tulloch, Yadin Dudai, and Paul~A.T. Kelly.
\newblock Memory reconsolidation: Sensitivity of spatial memory to inhibition
  of protein synthesis in dorsal hippocampus during encoding and retrieval.
\newblock {\em Neuron}, 50(3):479--489, 2006.

\bibitem{NairetalCoRR-15}
Arun Naira, Praveen Srinivasana, Sam Blackwella, Cagdas Alciceka, Rory Fearona,
  Alessandro~De Mariaa, Vedavyas Panneershelvama, Mustafa Suleymana, Charles
  Beattiea, Stig Petersena, Shane Legga, Volodymyr Mniha, Koray Kavukcuoglua,
  and David Silver.
\newblock Massively parallel methods for deep reinforcement learning.
\newblock {\em CoRR}, arXiv:1507.04296, 2015.

\bibitem{NarasimhanetalJAIR-18}
Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola.
\newblock Grounding language for transfer in deep reinforcement learning.
\newblock {\em Journal of Artificial Intelligence Research}, 63:849--874, 2018.

\bibitem{NeunuebelandKnierimNEURON-14}
Joshua~P. Neunuebel and James~J. Knierim.
\newblock {CA3} retrieves coherent representations from degraded input: Direct
  evidence for {CA3} pattern completion and dentate gyrus pattern separation.
\newblock {\em Neuron}, 81(2):416--427, 2014.

\bibitem{OishietalNEUROIMAGE-08}
Kenichi Oishi, Karl Zilles, Katrin Amunts, Andreia Faria, Hangyi Jiang, Xin Li,
  Kazi Akhter, Kegang Hua, Roger Woods, Arthur~W. Toga, G.~Bruce Pike, Pedro
  Rosa-Neto, Alan Evans, Jiangyang Zhang, Hao Huang, Michael~I. Miller,
  Peter~C.M. van Zijl, John Mazziotta, and Susumu Mori.
\newblock Human brain white matter atlas: Identification and assignment of
  common anatomical structures in superficial white matter.
\newblock {\em NeuroImage}, 43(3):447--457, 2008.

\bibitem{OlafsdottirCURRENT-BIOLOGY-18}
H.~Freyja \'{O}lafsd\'{o}ttir, Daniel Bush, and Caswell Barry.
\newblock The role of hippocampal replay in memory and planning.
\newblock {\em Current Biology}, 28(1):R37--R50, 2018.

\bibitem{OReillySCIENCE-06}
Randall~C. O'Reilly.
\newblock Biologically based computational models of high-level cognition.
\newblock {\em Science}, 314:91--94, 2006.

\bibitem{OReillyetalCS-15}
Randall~C. O'Reilly, Rajan Bhattacharyya, Michael~D. Howard, and Nicholas Ketz.
\newblock Complementary learning systems.
\newblock {\em Cognitive Science}, 38(6):1229--1248, 2014.

\bibitem{OReillyandFrankNC-06}
Randall~C. O'Reilly and Michael~J. Frank.
\newblock Making working memory work: A computational model of learning in the
  prefrontal cortex and basal ganglia.
\newblock {\em Neural Computation}, 18:283--328, 2006.

\bibitem{o2007pvlv}
Randall~C O'Reilly, Michael~J Frank, Thomas~E Hazy, and Brandon Watz.
\newblock Pvlv: the primary value and learned value pavlovian learning
  algorithm.
\newblock {\em Behavioral neuroscience}, 121(1):31, 2007.

\bibitem{OReillyetalLEABRA-16}
Randall~C. O'Reilly, Thomas~E. Hazy, and Seth~A. Herd.
\newblock The {Leabra} cognitive architecture: {H}ow to play 20 principles with
  nature and win!
\newblock In Susan E.~F. Chipman, editor, {\em The Oxford Handbook of Cognitive
  Science}, Oxford Handbooks, pages 91--115. Oxford University Press, 2016.

\bibitem{OReillyetalCCN-12}
Randall~C. O'Reilly, Yuko Munakata, Michael~J. Frank, Thomas~E. Hazy, and
  Contributors.
\newblock {\em Computational Cognitive Neuroscience}.
\newblock Wiki Book, 1st Edition, 2012.

\bibitem{OReillyetalTACO-14}
Randall~C. O'Reilly, Alex~A. Petrov, Jonathan~D. Cohen, Christian~J. Lebiere,
  Seth~A. Herd, and Trent Kriete.
\newblock How limited systematicity emerges: A computational cognitive
  neuroscience approach.
\newblock In Paco Calvo and John Symons, editors, {\em The Architecture of
  Cognition}, pages 191--224. MIT Press, Cambridge, Massachusetts, 2014.

\bibitem{PascanuetalCoRR-17}
Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing,
  S{\'{e}}bastien Racani{\`{e}}re, David~P. Reichert, Theophane Weber, Daan
  Wierstra, and Peter Battaglia.
\newblock Learning model-based planning from scratch.
\newblock {\em CoRR}, arXiv:1707.06170, 2017.

\bibitem{PashevichetalCoRR-18}
Alexander Pashevich, Danijar Hafner, James Davidson, Rahul Sukthankar, and
  Cordelia Schmid.
\newblock Modulated policy hierarchies.
\newblock {\em CoRR}, arXiv:1812.00025, 2018.

\bibitem{PlattandSpelkeCOiN-09}
M.L. Platt and E.~S Spelke.
\newblock What can developmental and comparative cognitive neuroscience tell us
  about the adult human brain?
\newblock {\em Current Opinion in Neurobiology}, 19(1):1--5, 2009.

\bibitem{PortuguesetalNEURON-14}
Ruben Portugues, Claudia~E. Feierstein, Florian Engert, and Michael~B. Orger.
\newblock Whole-brain activity maps reveal stereotyped, distributed networks
  for visuomotor behavior.
\newblock {\em Neuron}, 81:1328--1343, 2014.

\bibitem{LampinenetalCoRR-19}
Sebastien Racaniere, Andrew~K. Lampinen, Adam Santoro, David~P. Reichert, Vlad
  Firoiu, and Timothy~P. Lillicrap.
\newblock Automated curricula through setter-solver interactions.
\newblock {\em CoRR}, arXiv:1909.12892, 2019.

\bibitem{RakiketalTCN-09}
Pasko Rakik, Jon Arellano, and Joshua Breunig.
\newblock Development of the primate cerebral cortex.
\newblock In Michael~S. Gazzaniga, editor, {\em The Cognitive Neurosciences,
  4th Edition}, pages 7--28. The MIT Press, Cambridge, MA, 2009.

\bibitem{RasmussenetalPLoS-ONE-17}
Daniel Rasmussen, Aaron Voelker, and Chris Eliasmith.
\newblock A neural model of hierarchical reinforcement learning.
\newblock {\em PLOS ONE}, 12(7):1--39, 2017.

\bibitem{RibasFernandesNEURON-11}
J.~J. Ribas-Fernandes, A.~Solway, C.~Diuk, J.~T. McGuire, A.~G. Barto, Y.~Niv,
  and M.~M. Botvinick.
\newblock A neural signature of hierarchical reinforcement learning.
\newblock {\em Neuron}, 71(2):370--379, 2011.

\bibitem{RobinsCONNECTION-SCIENCE-95}
Anthony Robins.
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock {\em Connection Science}, 7(2):123--146, 1995.

\bibitem{RosatietalEVOLUTIONARY-PSYCHOLOGY-14}
Alexandra~G. Rosati, Victoria Wobber, Kelly Hughes, and Laurie~R. Santos.
\newblock Comparative developmental psychology: How is human cognitive
  development unique?
\newblock {\em Evolutionary Psychology}, 12(2):147470491401200211, 2014.

\bibitem{RoschNATURAL-CATEGORIES-95}
Eleanor Rosch.
\newblock Cognitive reference points.
\newblock {\em Cognitive Psychology}, 7(4):532--547, 1975.

\bibitem{RoschNATURAL-CATEGORIES-91}
Eleanor~H. Rosch.
\newblock Natural categories.
\newblock {\em Cognitive Psychology}, 4(3):328--350, 1973.

\bibitem{RoseetalCHILD-DEVELOPMENT-09}
S.~A. Rose, J.~F. Feldman, and J.~J. Jankowski.
\newblock A cognitive approach to the development of early language.
\newblock {\em Child Development}, 80(1):134--150, 2009.

\bibitem{RueckemannandBuffaloNATURE-2017}
Jon~W. Rueckemann and Elizabeth~A. Buffalo.
\newblock Auditory landscape on the cognitive map.
\newblock {\em Nature}, 543:631, 2017.

\bibitem{SahnietalCoRR-17}
Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and Charles~L. Isbell.
\newblock Learning to compose skills.
\newblock {\em CoRR}, arXiv:1711.11289, 2017.

\bibitem{SanchezetalCoRR-18}
Alvaro Sanchez{-}Gonzalez, Nicolas Heess, Jost~Tobias Springenberg, Josh Merel,
  Martin~A. Riedmiller, Raia Hadsell, and Peter Battaglia.
\newblock Graph networks as learnable physics engines for inference and
  control.
\newblock {\em CoRR}, arXiv:1806.01242, 2018.

\bibitem{SantoroetalNIPS-17}
Adam Santoro, David Raposo, David~G Barrett, Mateusz Malinowski, Razvan
  Pascanu, Peter Battaglia, and Timothy Lillicrap.
\newblock A simple neural network module for relational reasoning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 4967--4976. Curran Associates, Inc., 2017.

\bibitem{SchapiroetalNATURE-COMMUNICATIONS-18}
Anna~C Schapiro, Elizabeth~A McDevitt, Timothy~T Rogers, Sara~C Mednick, and
  Kenneth~A Norman.
\newblock Human hippocampal replay during rest prioritizes weakly learned
  information and predicts memory performance.
\newblock {\em Nature communications}, 9(1):3920, 2018.

\bibitem{SchauletalCoRR-15}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock {\em CoRR}, arXiv:1511.05952, 2015.

\bibitem{SchlichtingandPrestonCOiBS}
M.~L. Schlichting and A.~R. Preston.
\newblock Memory integration: neural mechanisms and implications for behavior.
\newblock {\em Current Opinion in Behavioral Science}, 1:1--8, 2015.

\bibitem{Schmidt-HieberetalNATURE-04}
C.~Schmidt-Hieber, P.~Jonas, and J.~Bischofberger.
\newblock Enhanced synaptic plasticity in newly generated granule cells of the
  adult hippocampus.
\newblock {\em Nature}, 429(6988):184--187, 2004.

\bibitem{SchuckandNivDOI-18}
NW~Schuck and Y~Niv.
\newblock Sequential replay of non-spatial task states in the human
  hippocampus. biorxiv.
\newblock {\em DOI}, 10:315978, 2018.

\bibitem{SeldinetalICSS-01}
Yevgeny Seldin, Gill Bejerano, and Naftali Tishby.
\newblock Unsupervised segmentation and classification of mixtures of
  {M}arkovian sources.
\newblock In {\em Proceedings of the 33rd Symposium on the Interface of
  Computing Science and Statistics}, 2001.

\bibitem{SeldinetalICML-01}
Yevgeny Seldin, Gill Bejerano, and Naftali Tishby.
\newblock Unsupervised sequence segmentation by a mixture of switching variable
  memory {M}arkov sources.
\newblock In {\em Proceedings of the Eighteenth International Conference on
  Machine Learning (ICML 2001)}, pages 513--520, 2001.

\bibitem{SinghandKohliSNAPL-17}
Rishabh Singh and Pushmeet Kohli.
\newblock {AP:Artificial Programming}.
\newblock In {\em Summit on Advances in Programming Languages 2017}, 2017.

\bibitem{SpaldingetalCELL-13}
Kirsty~L. Spalding, Olaf Bergmann, Kanar Alkass, Samuel Bernard, Mehran
  Salehpour, Hagen~B. Huttner, Emil Bostr\"{o}m, Isabelle Westerlund, Celine
  Vial, Bruce~A. Buchholz, G\"{o}ran Possnert, Deborah~C. Mash, Henrik Druid,
  and Jonas Fris\'{e}n.
\newblock Dynamics of hippocampal neurogenesis in adult humans.
\newblock {\em Cell}, 153:1219--1227, 2013.

\bibitem{StachenfeldetalNATURE-17}
Kimberly~L. Stachenfeld, Matthew~M. Botvinick, and Samuel~J. Gershman.
\newblock The hippocampus as a predictive map.
\newblock {\em Nature Neuroscience}, 20:1643, 2017.

\bibitem{SwansonTiN-95}
Larry~W. Swanson.
\newblock Mapping the human brain: past, present, and future.
\newblock {\em Trends in Neurosciences}, 18(11):471--474, 1995.

\bibitem{sylvester2009anticipatory}
CM~Sylvester, GL~Shulman, AI~Jack, and M~Corbetta.
\newblock Anticipatory and stimulus-evoked blood oxygenation level-dependent
  modulations related to spatial attention reflect a common additive signal.
\newblock {\em The Journal of neuroscience: the official journal of the Society
  for Neuroscience}, 29(34):10671, 2009.

\bibitem{TravisetalHBM-15}
Travis, Katherine E., Yael Leitner, Heidi~M. Feldman, and Michal Ben-Shachar.
\newblock Cerebellar white matter pathways are associated with reading skills
  in children and adolescents.
\newblock {\em Human Brain Mapping}, 36(4):1536--1553, 2015.

\bibitem{VarelaThompsonRoschTHE_EMBODIED_MIND-91}
F.J. Varela, Eleanor. Rosch, and E.~Thompson.
\newblock {\em The Embodied Mind: Cognitive Science and Human Experience}.
\newblock MIT Press, 1991.

\bibitem{vonUexk1926theoretical}
J.~von Uexk\"{u}ll and D.L. Mackinnon.
\newblock {\em Theoretical Biology}.
\newblock {International Library of Psychology, Philosophy and Scientific
  Method}. K. Paul, Trench, Trubner \& Company Limited, 1926.

\bibitem{VyletaetalELIFE-16}
Nicholas~P Vyleta, Carolina Borges-Merjane, and Peter Jonas.
\newblock Plasticity-dependent, full detonation at hippocampal mossy
  fiber–{CA3} pyramidal neuron synapses.
\newblock {\em eLife}, 5:e17977, 2016.

\bibitem{WakanaetalRADIOLOGY-04}
S.~Wakana, H.~Jiang, L.~M. Nagae-Poetscher, P.~C. van Zijl, and S.~Mori.
\newblock Fiber tract-based atlas of human white matter anatomy.
\newblock {\em Radiology}, 230(1):77--87, 2004.

\bibitem{WangetalNATURE-NEUROSCIENCE-18}
Jane~X. Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert
  Soyer, Joel~Z. Leibo, Demis Hassabis, and Matthew Botvinick.
\newblock Prefrontal cortex as a meta-reinforcement learning system.
\newblock {\em Nature Neuroscience}, 21:860--868, 2018.

\bibitem{WangetalCoRR-17}
Ke~Wang, Rishabh Singh, and Zhendong Su.
\newblock Dynamic neural program embedding for program repair.
\newblock {\em CoRR}, arXiv:1711.07163, 2017.

\bibitem{WangandWangFiP-19}
Wei Wang and Guang-Zhong Wang.
\newblock Understanding molecular mechanisms of the brain through
  transcriptomics.
\newblock {\em Frontiers in physiology}, 10:214--214, 2019.

\bibitem{WeberetalCoRR-17}
Theophane Weber, S{\'{e}}bastien Racani{\`{e}}re, David~P. Reichert, Lars
  Buesing, Arthur Guez, Danilo~Jimenez Rezende, Adri{\`{a}}~Puigdom{\`{e}}nech
  Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter
  Battaglia, David Silver, and Daan Wierstra.
\newblock Imagination-augmented agents for deep reinforcement learning.
\newblock {\em CoRR}, arXiv:1707.06203, 2017.

\bibitem{WestonetalCoRR-14}
Jason Weston, Sumit Chopra, and Antoine Bordes.
\newblock Memory networks.
\newblock {\em CoRR}, arXiv:1410.3916, 2014.

\bibitem{WillemetBRAIN-SCIENCE-12}
Romain Willemet.
\newblock Understanding the evolution of mammalian brain structures; the need
  for a (new) cerebrotype approach.
\newblock {\em Brain sciences}, 2:203--224, 2012.

\bibitem{WillshawetalPTRS_B-15}
D.~J. Willshaw, P.~Dayan, and R.~G.~M. Morris.
\newblock Memory, modelling and marr: a commentary on marr (1971) simple
  memory: a theory of archicortex.
\newblock {\em Philosophical Transactions of the Royal Society B: Biological
  Sciences}, 370(1666):20140383, 2015.

\bibitem{WilsonandMcNaughtonSCIENCE-94}
M.~A. Wilson and B.L. McNaughton.
\newblock Reactivation of hippocampal ensemble memories during sleep.
\newblock {\em Science}, 265(5172):676--679, 1994.

\bibitem{WiskottetalHIPPOCAMPUS-06}
L.~Wiskott, M.~J. Rasch, and G.~Kempermann.
\newblock A functional hypothesis for adult hippocampal neurogenesis: avoidance
  of catastrophic interference in the dentate gyrus.
\newblock {\em Hippocampus}, 16(3):329--343, 2006.

\bibitem{Higher_Cortical_Functions_Association}
Anthony Wright.
\newblock Higher cortical functions: Association and executive processing.
\newblock In {\em Neuroscience Online: An electronic textbook for the
  neurosciences}. The University of Texas McGovern Medical School, 1997.

\bibitem{YizharetalNEURON-11}
O.~Yizhar, L.E. Fenno, T.J. Davidson, M.~Mogri, and K.~Deisseroth.
\newblock Optogenetics in neural systems.
\newblock {\em Neuron}, 71:9--34, 2011.

\bibitem{ZhangetalNATURE-10}
F.~Zhang, V.~Gradinaru, A.R. Adamantidis, R.~Durand, R.D. Airan, L.~de~Lecea,
  and K.~Deisseroth.
\newblock Optogenetic interrogation of neural circuits: technology for probing
  mammalian brain structures.
\newblock {\em Nature Protocols}, 5:439--56, 2010.

\bibitem{ZhengetalCELL-18}
Zhihao Zheng, J.~Scott Lauritzen, Eric Perlman, Camenzind~G. Robinson, Matthew
  Nichols, Daniel Milkie, Omar Torrens, John Price, Corey~B. Fisher, Nadiya
  Sharifi, Steven~A. Calle-Schuler, Lucia Kmecova, Iqbal~J. Ali, Bill Karsh,
  Eric~T. Trautman, John Bogovic, Philipp Hanslovsky, Gregory S. X.~E.
  Jefferis, Michael Kazhdan, Khaled Khairy, Stephan Saalfeld, Richard~D.
  Fetter, and Davi~D. Bock.
\newblock A complete electron microscopy volume of the brain of adult
  drosophila melanogaster.
\newblock {\em Cell}, 174:1--14, 2018.

\end{thebibliography}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
